{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973df272",
   "metadata": {},
   "source": [
    "# Text Mining \n",
    "\n",
    "## 1. Pr√©processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"flickr_data_clustered.parquet\")\n",
    "print(f\"Donn√©es charg√©es : {len(df)} photos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71074a77",
   "metadata": {},
   "source": [
    "### D√©finition des stopwords\n",
    "\n",
    "On d√©finit manuellement une liste de stopwords, contenant les mots non-pertinents pour identifier les lieux touristiques de Lyon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21580870",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_from_method1 = {\n",
    "    \"lyon\", \"france\", \"img\", \"jpg\", \"uploaded\", \"europe\", \"square\", \"iphone\", \"instagram\",\n",
    "    \"franca\", \"rhonealpes\", \"geotagged\", \"janvier\", \"fevrier\", \"mars\", \"avril\", \"mai\", \"juin\",\n",
    "    \"juillet\", \"aout\", \"septembre\", \"octobre\", \"novembre\", \"decembre\", \"interieur\", \"live\",\n",
    "    \"des\", \"squareformat\", \"dsc\", \"iphoneography\", \"art\", \"instagramapp\", \"foursquare\",\n",
    "    \"venue\", \"japan\", \"flickrmobile\", \"french\", \"francia\", \"dscf\", \"frankrijk\", \"frankreich\",\n",
    "    \"flickriosapp\", \"touch\"\n",
    "}\n",
    "\n",
    "stopwords_from_method2 = {\n",
    "    \"ngc\", \"paysage\", \"landscape\", \"upload\", \"ios\", \"flickr\", \"mobile\", \"app\", \"filter\",\n",
    "    \"blackandwhite\", \"black\", \"white\", \"phone\", \"noiret\", \"noir\", \"blanc\", \"the\", \"nos\",\n",
    "    \"canon\", \"mmf\", \"monochrome\", \"alpes\", \"rhones\"\n",
    "}\n",
    "\n",
    "# Unifier les 2 ensembles\n",
    "stopwords = stopwords_from_method1.union(stopwords_from_method2)\n",
    "\n",
    "print(f\"Nombre total de stopwords : {len(stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da970c1",
   "metadata": {},
   "source": [
    "### Fonction de nettoyage du texte\n",
    "\n",
    "On cr√©e une fonction qui :\n",
    "1. Concat√®ne title et tags\n",
    "2. Met en minuscules\n",
    "3. Supprime les accents (√© ‚Üí e, √ß ‚Üí c)\n",
    "4. Supprime la ponctuation et caract√®res sp√©ciaux\n",
    "5. Filtre les stopwords et mots trop courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Gestion de la librairie wordsegment pour d√©couper les hashtags (ex: #fetedeslumieres)\n",
    "try:\n",
    "    from wordsegment import load, segment\n",
    "except ImportError:\n",
    "    print(\"Installation de la librairie 'wordsegment'...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wordsegment\"])\n",
    "    from wordsegment import load, segment\n",
    "\n",
    "# Chargement du dictionnaire (n√©cessaire une seule fois)\n",
    "try:\n",
    "    load()\n",
    "except:\n",
    "    pass # D√©j√† charg√© ou erreur de chargement\n",
    "\n",
    "def clean_text(title, tags):\n",
    "    \"\"\"\n",
    "    Nettoie et combine title et tags en une liste de mots pertinents.\n",
    "    Inclut une segmentation des mots concat√©n√©s (hashtags).\n",
    "    \n",
    "    Args:\n",
    "        title: Titre de la photo (str ou NaN)\n",
    "        tags: Tags de la photo (str ou NaN)\n",
    "    \n",
    "    Returns:\n",
    "        Liste de mots nettoy√©s\n",
    "    \"\"\"\n",
    "    # Combiner title et tags\n",
    "    text = \"\"\n",
    "    if isinstance(title, str):\n",
    "        text += title + \" \"\n",
    "    if isinstance(tags, str):\n",
    "        text += tags\n",
    "    \n",
    "    if not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # 1. Minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Supprimer accents et caract√®res sp√©ciaux Unicode\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "    \n",
    "    # 3. Ne garder que lettres et espaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # 4. Split initial\n",
    "    words = text.split()\n",
    "    \n",
    "    # 5. Segmentation (d√©coupage des mots coll√©s comme 'fetedeslumieres')\n",
    "    expanded_words = []\n",
    "    for w in words:\n",
    "        # segment() d√©coupe \"fetedeslumieres\" en [\"fete\", \"des\", \"lumieres\"]\n",
    "        # et laisse \"lyon\" en [\"lyon\"]\n",
    "        expanded_words.extend(segment(w))\n",
    "    \n",
    "    # 6. Filtrer stopwords et mots courts\n",
    "    final_words = [w for w in expanded_words if w not in stopwords and len(w) > 2]\n",
    "    \n",
    "    # 7. Supprimer les doublons tout en pr√©servant l'ordre\n",
    "    unique_words = list(dict.fromkeys(final_words))\n",
    "    \n",
    "    return unique_words\n",
    "\n",
    "# Test de la fonction\n",
    "test_title = \"[Lyon] F√™te des lumi√®res 2024\"\n",
    "test_tags = \"fetedeslumieres, lyon, onlylyon, rhonealpes\"\n",
    "print(\"Test de nettoyage avec segmentation :\")\n",
    "print(f\"Input : '{test_title}' + '{test_tags}'\")\n",
    "print(f\"Output : {clean_text(test_title, test_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a7f3a",
   "metadata": {},
   "source": [
    "### Application du nettoyage\n",
    "\n",
    "On applique la fonction de nettoyage sur toutes les photos pour cr√©er une colonne unique `cleaned_text`.\n",
    "\n",
    "**Note** : Cette √©tape peut prendre quelques minutes avec `wordsegment`. Une barre de progression s'affichera pour suivre l'avancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer tqdm pour la barre de progression\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Nettoyage en cours\")\n",
    "\n",
    "# Appliquer le nettoyage sur title + tags avec barre de progression\n",
    "df['cleaned_text'] = df.progress_apply(\n",
    "    lambda row: clean_text(row['title'], row['tags']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Statistiques sur le nettoyage\n",
    "total_photos = len(df)\n",
    "photos_avec_mots = (df['cleaned_text'].str.len() > 0).sum()\n",
    "photos_sans_mots = total_photos - photos_avec_mots\n",
    "\n",
    "print(f\"\\n‚úì Nettoyage termin√©\")\n",
    "print(f\"  - Photos avec mots : {photos_avec_mots} ({photos_avec_mots/total_photos*100:.1f}%)\")\n",
    "print(f\"  - Photos sans mots : {photos_sans_mots} ({photos_sans_mots/total_photos*100:.1f}%)\")\n",
    "print(f\"  - Nombre moyen de mots/photo : {df['cleaned_text'].str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d848f",
   "metadata": {},
   "source": [
    "### V√©rification du r√©sultat\n",
    "\n",
    "On affiche quelques exemples pour v√©rifier que le nettoyage fonctionne correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c706af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher quelques exemples\n",
    "sample = df[df['cleaned_text'].str.len() > 0].sample(n=10, random_state=42)\n",
    "\n",
    "print(\"Exemples de textes nettoy√©s :\\n\")\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"Title original : {row['title'][:80]}...\")\n",
    "    print(f\"Tags originaux : {row['tags'][:80] if isinstance(row['tags'], str) else 'N/A'}...\")\n",
    "    print(f\"Mots nettoy√©s  : {row['cleaned_text'][:10]}\")  # 10 premiers mots\n",
    "    print(f\"Cluster        : {row['cluster_hdbscan']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8ca8e",
   "metadata": {},
   "source": [
    "## 2. M√©thode 1 : Mot le plus fr√©quent par cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021bbe3",
   "metadata": {},
   "source": [
    "### Principe\n",
    "\n",
    "Pour chaque cluster, on identifie le mot le plus fr√©quent dans les textes nettoy√©s (titles + tags).\n",
    "\n",
    "**Hypoth√®se** : Le mot dominant repr√©sente le lieu ou le monument principal du cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057c0c3",
   "metadata": {},
   "source": [
    "### Fonction de calcul des mots dominants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_cluster_keywords(df, cluster_col='cluster_hdbscan', text_col='cleaned_text', top_k=1):\n",
    "    \"\"\"\n",
    "    Trouve le(s) mot(s) le(s) plus fr√©quent(s) pour chaque cluster.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame avec colonnes cluster et texte nettoy√©\n",
    "        cluster_col: Nom de la colonne contenant les labels de cluster\n",
    "        text_col: Nom de la colonne contenant les listes de mots nettoy√©s\n",
    "        top_k: Nombre de mots √† retourner par cluster\n",
    "    \n",
    "    Returns:\n",
    "        dict {cluster_id: mot_dominant}\n",
    "    \"\"\"\n",
    "    cluster_keywords = {}\n",
    "    \n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        if cluster_id == -1:\n",
    "            continue  # Ignorer le bruit\n",
    "        \n",
    "        # R√©cup√©rer tous les mots du cluster\n",
    "        cluster_data = df[df[cluster_col] == cluster_id]\n",
    "        all_words = []\n",
    "        \n",
    "        for words_list in cluster_data[text_col].dropna():\n",
    "            all_words.extend(words_list)\n",
    "        \n",
    "        # Si pas de mots, mettre \"unknown\"\n",
    "        if not all_words:\n",
    "            cluster_keywords[cluster_id] = \"unknown\"\n",
    "            continue\n",
    "        \n",
    "        # Compter les occurrences\n",
    "        word_counts = Counter(all_words)\n",
    "        \n",
    "        # Prendre le mot le plus fr√©quent\n",
    "        top_word = word_counts.most_common(top_k)[0][0]\n",
    "        cluster_keywords[cluster_id] = top_word\n",
    "    \n",
    "    return cluster_keywords\n",
    "\n",
    "# Calculer les mots-cl√©s dominants\n",
    "cluster_keywords = find_cluster_keywords(df)\n",
    "\n",
    "# Ajouter au DataFrame\n",
    "df['cluster_keyword'] = df['cluster_hdbscan'].map(cluster_keywords).fillna(\"noise\")\n",
    "\n",
    "print(f\"‚úì Mots-cl√©s calcul√©s pour {len(cluster_keywords)} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736abcd",
   "metadata": {},
   "source": [
    "### Statistiques et aper√ßu des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le r√©sum√© des clusters\n",
    "cluster_summary = df[df['cluster_hdbscan'] != -1].groupby('cluster_hdbscan').agg({\n",
    "    'cluster_keyword': 'first',\n",
    "    'id': 'count'\n",
    "}).rename(columns={'id': 'nb_photos'}).sort_values('nb_photos', ascending=False)\n",
    "\n",
    "n_clusters = len(cluster_summary)\n",
    "\n",
    "# Compter les mots-cl√©s par CLUSTER (pas par photo)\n",
    "keyword_cluster_counts = cluster_summary['cluster_keyword'].value_counts()\n",
    "n_unique_keywords = (keyword_cluster_counts == 1).sum()\n",
    "n_duplicate_keywords = len(keyword_cluster_counts[keyword_cluster_counts > 1])\n",
    "\n",
    "print(f\"üîç Qualit√© des mots-cl√©s:\")\n",
    "print(f\"  - Mots-cl√©s uniques: {n_unique_keywords}/{n_clusters} ({n_unique_keywords/n_clusters*100:.1f}%)\")\n",
    "print(f\"  - Mots-cl√©s dupliqu√©s: {n_duplicate_keywords} (partag√©s par plusieurs clusters)\")\n",
    "print(f\"  ‚ö†Ô∏è  Ambigu√Øt√©: {n_clusters - n_unique_keywords} clusters partagent un m√™me mot-cl√©\")\n",
    "\n",
    "# Top des mots-cl√©s les plus fr√©quents (candidats stopwords)\n",
    "print(f\"\\nüö´ Mots-cl√©s les plus fr√©quents (candidats pour stopwords):\")\n",
    "print(f\"    Ces mots-cl√©s dominent plusieurs clusters, ils sont donc peu distinctifs\\n\")\n",
    "for keyword, count in keyword_cluster_counts.head(20).items():\n",
    "    print(f\"    '{keyword}' : {count} clusters\")\n",
    "\n",
    "print(f\"\\nüìç Top 20 clusters par taille :\\n\")\n",
    "print(cluster_summary.head(20).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb3488",
   "metadata": {},
   "source": [
    "### Visualisation sur carte interactive\n",
    "\n",
    "On affiche les clusters sur une carte avec leurs mots-cl√©s dominants pour identifier visuellement les zones d'int√©r√™t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c77620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# √âchantillonner pour la performance\n",
    "sample_size = 30000\n",
    "sample = df.sample(n=min(sample_size, len(df)), random_state=0)\n",
    "\n",
    "# Cr√©er la carte\n",
    "m = folium.Map(\n",
    "    location=[df[\"lat\"].median(), df[\"long\"].median()],\n",
    "    zoom_start=12,\n",
    "    tiles=\"CartoDB positron\"\n",
    ")\n",
    "\n",
    "# Palette de couleurs\n",
    "palette = [\n",
    "    \"red\", \"blue\", \"green\", \"purple\", \"orange\",\n",
    "    \"darkred\", \"lightred\", \"beige\", \"darkblue\",\n",
    "    \"darkgreen\", \"cadetblue\", \"darkpurple\",\n",
    "    \"pink\", \"lightblue\", \"lightgreen\",\n",
    "    \"gray\", \"black\", \"lightgray\"\n",
    "]\n",
    "\n",
    "# Ajouter les points\n",
    "for _, r in sample.iterrows():\n",
    "    cluster = r[\"cluster_hdbscan\"]\n",
    "    \n",
    "    if cluster == -1:\n",
    "        continue\n",
    "    else:\n",
    "        color = palette[cluster % len(palette)]\n",
    "        keyword = r[\"cluster_keyword\"]\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[r[\"lat\"], r[\"long\"]],\n",
    "        radius=2,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.6,\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>Keyword:</b> {keyword}<br/>\n",
    "               <b>Cluster:</b> {cluster}<br/>\n",
    "               <a href=\"{r.get('url', '#')}\" target=\"_blank\">Open Flickr</a>\"\"\",\n",
    "            max_width=250\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "# Titre sur la carte\n",
    "title_html = '''\n",
    "<div style=\"position: fixed; top: 10px; left: 50px; width: 400px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "     <b>M√©thode 1 : Mot le plus fr√©quent par cluster</b>\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Sauvegarder\n",
    "m.save(\"output/text_mining_method1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edaaa74",
   "metadata": {},
   "source": [
    "### Analyse des r√©sultats\n",
    "\n",
    "**Observations** :\n",
    "\n",
    "En explorant la carte, on peut identifier les mots-cl√©s dominants pour chaque zone touristique. Certains mots sont pertinents (ex: \"croixrousse\", \"bellecour\", \"jacobins\"), d'autres sont trop g√©n√©riques ou non-informatifs.\n",
    "\n",
    "**Limites de cette m√©thode** :\n",
    "\n",
    "1. **Sensibilit√© aux stopwords** : La qualit√© d√©pend fortement de la liste de stopwords. Des mots non-pertinents peuvent dominer si on ne les filtre pas manuellement, ce qui est fastidieux et propice aux erreurs.\n",
    "\n",
    "2. **Mot unique par cluster** : Un seul mot ne suffit pas toujours √† caract√©riser un lieu (ex: \"Part Dieu\" ‚Üí seulement \"part\" ou \"dieu\").\n",
    "\n",
    "3. **Approche it√©rative** : N√©cessite d'inspecter visuellement les r√©sultats et d'ajuster manuellement les stopwords pour am√©liorer la qualit√©.\n",
    "\n",
    "4. **Pas de pond√©ration contextuelle** : Tous les mots ont le m√™me poids, qu'ils soient sp√©cifiques (ex: \"fourviere\") ou g√©n√©riques (ex: \"rue\", \"statue\").\n",
    "\n",
    "**Am√©lioration possible** : Utiliser TF-IDF pour pond√©rer l'importance des mots en fonction de leur sp√©cificit√© √† chaque cluster, plut√¥t que simplement compter les fr√©quences brutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914ba61",
   "metadata": {},
   "source": [
    "## 3. M√©thode 2 : TF-IDF\n",
    "\n",
    "Pour pallier les limites de la simple fr√©quence, on utilise **TF-IDF** (Term Frequency - Inverse Document Frequency).\n",
    "\n",
    "**Principe** : TF-IDF donne un score √©lev√© aux mots :\n",
    "- Fr√©quents dans un cluster sp√©cifique (TF)\n",
    "- Rares dans les autres clusters (IDF)\n",
    "\n",
    "Cela permet de capturer les mots **distinctifs** plut√¥t que simplement fr√©quents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7d76c",
   "metadata": {},
   "source": [
    "### Pr√©paration des textes\n",
    "\n",
    "On transforme chaque photo en un texte, puis on concat√®ne toutes les photos d‚Äôun cluster en un seul document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Texte par photo\n",
    "df['combined_text'] = df['cleaned_text'].apply(lambda words: ' '.join(words))\n",
    "\n",
    "# 2) Document par cluster\n",
    "cluster_documents = df[df['cluster_hdbscan'] != -1].groupby('cluster_hdbscan')['combined_text'] \\\n",
    "    .apply(lambda texts: ' '.join(texts)) \\\n",
    "    .reset_index()\n",
    "\n",
    "cluster_documents.columns = ['cluster', 'document']\n",
    "cluster_documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f044769",
   "metadata": {},
   "source": [
    "On regarde le nombre de clusters et la taille moyenne des documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff505edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre de clusters : {len(cluster_documents)}\")\n",
    "print(f\"Taille moyenne des documents : {cluster_documents['document'].str.len().mean():.0f} caract√®res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a876c47",
   "metadata": {},
   "source": [
    "### Construction de la matrice TF‚ÄëIDF\n",
    "\n",
    "On garde uniquement les mots utiles :\n",
    "- `min_df=2` : le mot doit appara√Ætre dans au moins 2 clusters\n",
    "- `max_df=0.8` : on supprime les mots trop communs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(cluster_documents['document'])\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print(f\"Matrice TF‚ÄëIDF : {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70092df3",
   "metadata": {},
   "source": [
    "### Extraction des top mots par cluster\n",
    "\n",
    "On cr√©e une fonction r√©utilisable pour extraire les mots les plus repr√©sentatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_terms_for_cluster(cluster_idx, top_n=3):\n",
    "    scores = tfidf_matrix[cluster_idx].toarray().flatten()\n",
    "    top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "    return feature_names[top_indices]\n",
    "\n",
    "# Associer les top mots √† chaque cluster\n",
    "tfidf_labels = {}\n",
    "\n",
    "for i in range(len(cluster_documents)):\n",
    "    cluster_id = cluster_documents.iloc[i]['cluster']\n",
    "    top_terms = get_top_terms_for_cluster(i, top_n=3)\n",
    "    tfidf_labels[cluster_id] = \" / \".join(top_terms)\n",
    "\n",
    "# Ajouter au DataFrame\n",
    "df['cluster_tfidf'] = df['cluster_hdbscan'].map(tfidf_labels).fillna(\"noise\")\n",
    "\n",
    "print(\"‚úì Mots-cl√©s TF‚ÄëIDF ajout√©s au dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c23da7a",
   "metadata": {},
   "source": [
    "### Visualisation des clusters (r√©sum√© final)\n",
    "\n",
    "On visualise les clusters sur une carte avec leurs labels TF‚ÄëIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700065b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# √âchantillonner pour la performance (comme avant)\n",
    "sample_size = 30000\n",
    "sample = df.sample(n=min(sample_size, len(df)), random_state=0)\n",
    "\n",
    "m = folium.Map(\n",
    "    location=[df[\"lat\"].median(), df[\"long\"].median()],\n",
    "    zoom_start=12,\n",
    "    tiles=\"CartoDB positron\"\n",
    ")\n",
    "\n",
    "palette = [\n",
    "    \"red\", \"blue\", \"green\", \"purple\", \"orange\",\n",
    "    \"darkred\", \"lightred\", \"beige\", \"darkblue\",\n",
    "    \"darkgreen\", \"cadetblue\", \"darkpurple\",\n",
    "    \"pink\", \"lightblue\", \"lightgreen\",\n",
    "    \"gray\", \"black\", \"lightgray\"\n",
    "]\n",
    "\n",
    "for _, r in sample.iterrows():\n",
    "    cluster = r[\"cluster_hdbscan\"]\n",
    "    if cluster == -1:\n",
    "        continue\n",
    "    \n",
    "    color = palette[cluster % len(palette)]\n",
    "    label = r[\"cluster_tfidf\"]\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[r[\"lat\"], r[\"long\"]],\n",
    "        radius=2,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.6,\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>TF‚ÄëIDF:</b> {label}<br/>\n",
    "               <b>Cluster:</b> {cluster}<br/>\n",
    "               <a href=\"{r.get('url', '#')}\" target=\"_blank\">Open Flickr</a>\"\"\",\n",
    "            max_width=250\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "title_html = '''\n",
    "<div style=\"position: fixed; top: 10px; left: 50px; width: 400px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "     <b>M√©thode 2 : TF‚ÄëIDF (top mots par cluster)</b>\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "m.save(\"output/text_mining_tfidf.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fe447",
   "metadata": {},
   "source": [
    "### Analyse des r√©sultats\n",
    "\n",
    "**Points forts** :\n",
    "- Les mots sont souvent plus sp√©cifiques que la m√©thode 1.\n",
    "- Moins d√©pendant des stopwords manuels.\n",
    "\n",
    "**Limites** :\n",
    "- La qualit√© d√©pend de la pr√©paration (cleaning + stopwords).\n",
    "\n",
    "TF‚ÄëIDF donne une base solide pour nommer les clusters, mais il faut toujours ajuster avec une inspection manuelle pour ajuster les stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8d6a5",
   "metadata": {},
   "source": [
    "## 4. M√©thode 3 : R√®gles d'Association"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566eb4ed",
   "metadata": {},
   "source": [
    "### Principe\n",
    "\n",
    "Les **r√®gles d'association** d√©couvrent des **combinaisons de mots** qui apparaissent fr√©quemment ensemble dans les clusters.\n",
    "\n",
    "**Diff√©rence avec les m√©thodes pr√©c√©dentes :**\n",
    "- M√©thode 1 : un seul mot dominant ‚Üí ex: \"basilique\"\n",
    "- M√©thode 2 (TF-IDF) : liste de mots distinctifs ‚Üí ex: \"basilique, fourviere, colline\"\n",
    "- **M√©thode 3** : associations entre mots ‚Üí ex: **\"basilique + fourviere\"** (vont souvent ensemble)\n",
    "\n",
    "On peut ainsi capturer les noms compos√©s et expressions (\"place bellecour\", \"parc tete d'or\") plut√¥t que des mots isol√©s.\n",
    "\n",
    "On commence avec l'algorithme Apriori, qui g√©n√®re des r√®gles du type `{mot1, mot2} ‚Üí {mot3}` avec des scores de confiance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14f79a",
   "metadata": {},
   "source": [
    "### Pr√©paration des transactions\n",
    "\n",
    "On transforme chaque cluster en une \"transaction\" (liste de mots uniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les transactions : 1 transaction = tous les mots d'un cluster\n",
    "transactions = []\n",
    "\n",
    "for cluster_id in sorted(df['cluster_hdbscan'].unique()):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    \n",
    "    # R√©cup√©rer tous les mots du cluster\n",
    "    cluster_data = df[df['cluster_hdbscan'] == cluster_id]\n",
    "    all_words = []\n",
    "    \n",
    "    for words_list in cluster_data['cleaned_text'].dropna():\n",
    "        all_words.extend(words_list)\n",
    "    \n",
    "    # Garder les mots uniques\n",
    "    unique_words = list(set(all_words))\n",
    "    transactions.append(unique_words)\n",
    "\n",
    "print(f\"‚úì {len(transactions)} transactions cr√©√©es\")\n",
    "print(f\"  Exemple (cluster 0) : {transactions[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187ce2c",
   "metadata": {},
   "source": [
    "L'algorithme Apriori n√©cessite un format binaire : chaque ligne = 1 transaction, chaque colonne = 1 mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(f\"Matrice encod√©e : {df_encoded.shape}\")\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1b342",
   "metadata": {},
   "source": [
    "### Apriori avec plusieurs param√®tres\n",
    "\n",
    "Le **support** minimal d√©finit la fr√©quence minimale d'apparition d'une combinaison de mots.\n",
    "\n",
    "On teste plusieurs valeurs pour trouver le bon √©quilibre :\n",
    "- Trop bas ‚Üí trop de r√®gles (bruit)\n",
    "- Trop haut ‚Üí pas assez de r√®gles (perte d'info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ae075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import numpy as np\n",
    "\n",
    "support_values = [0.10, 0.125, 0.15, 0.20, 0.25, 0.30]\n",
    "results = []\n",
    "\n",
    "for min_sup in support_values:\n",
    "    print(f\"D√©but pour min_support = {min_sup}\")\n",
    "    \n",
    "    frequent_itemsets = apriori(\n",
    "        df_encoded,\n",
    "        min_support=min_sup,\n",
    "        use_colnames=True,\n",
    "        max_len=3\n",
    "    )\n",
    "    \n",
    "    # Ajouter length\n",
    "    if len(frequent_itemsets) > 0:\n",
    "        frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(len)\n",
    "        max_len = frequent_itemsets['length'].max()\n",
    "    else:\n",
    "        max_len = 0\n",
    "    \n",
    "    # G√©n√©rer des r√®gles si possible\n",
    "    if len(frequent_itemsets) > 0:\n",
    "        rules = association_rules(\n",
    "            frequent_itemsets,\n",
    "            metric=\"confidence\",\n",
    "            min_threshold=0.6,\n",
    "            num_itemsets=len(transactions)\n",
    "        )\n",
    "        mean_lift = rules['lift'].mean() if len(rules) > 0 else 0\n",
    "        mean_conf = rules['confidence'].mean() if len(rules) > 0 else 0\n",
    "    else:\n",
    "        rules = pd.DataFrame()\n",
    "        mean_lift = 0\n",
    "        mean_conf = 0\n",
    "    \n",
    "    # Couverture : % de clusters ayant au moins un itemset de taille >=2\n",
    "    if len(frequent_itemsets) > 0:\n",
    "        itemsets_2plus = frequent_itemsets[frequent_itemsets['length'] >= 2]['itemsets']\n",
    "        covered = 0\n",
    "        for cluster_words in transactions:\n",
    "            if any(itemset.issubset(set(cluster_words)) for itemset in itemsets_2plus):\n",
    "                covered += 1\n",
    "        coverage = covered / len(transactions)\n",
    "    else:\n",
    "        coverage = 0\n",
    "    \n",
    "    results.append({\n",
    "        'min_support': min_sup,\n",
    "        'n_itemsets': len(frequent_itemsets),\n",
    "        'max_length': max_len,\n",
    "        'mean_lift': mean_lift,\n",
    "        'mean_confidence': mean_conf,\n",
    "        'coverage': coverage\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚Üí itemsets: {len(frequent_itemsets)} | rules: {len(rules)} | coverage: {coverage:.2f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedeef50",
   "metadata": {},
   "source": [
    "On visualise le r√©sultat de l'exploration avec plusieurs valeurs de param√®tres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df775d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "# 1) Quantit√© d‚Äôitemsets\n",
    "ax[0][0].plot(results_df['min_support'], results_df['n_itemsets'], marker='o', linewidth=2)\n",
    "ax[0][0].set_title(\"Quantit√© d'itemsets\")\n",
    "ax[0][0].set_xlabel(\"min_support\")\n",
    "ax[0][0].set_ylabel(\"Nombre d'itemsets\")\n",
    "ax[0][0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2) Lift\n",
    "ax[0][1].plot(results_df['min_support'], results_df['mean_lift'], marker='s', label='Lift moyen')\n",
    "ax[0][1].set_title(\"Qualit√© des r√®gles : Lift\")\n",
    "ax[0][1].set_xlabel(\"min_support\")\n",
    "ax[0][1].set_ylabel(\"Valeur moyenne\")\n",
    "ax[0][1].legend()\n",
    "ax[0][1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3) Confidence\n",
    "ax[1][0].plot(results_df['min_support'], results_df['mean_confidence'], marker='^', label='Confidence moyenne')\n",
    "ax[1][0].set_title(\"Qualit√© des r√®gles : Confidence\")\n",
    "ax[1][0].set_xlabel(\"min_support\")\n",
    "ax[1][0].set_ylabel(\"Valeur moyenne\")\n",
    "ax[1][0].legend()\n",
    "ax[1][0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4) Couverture\n",
    "ax[1][1].plot(results_df['min_support'], results_df['coverage'], marker='o', color='green')\n",
    "ax[1][1].set_title(\"Couverture des clusters\")\n",
    "ax[1][1].set_xlabel(\"min_support\")\n",
    "ax[1][1].set_ylabel(\"Proportion couverte\")\n",
    "ax[1][1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84b647",
   "metadata": {},
   "source": [
    "### Analyse des graphiques et choix des param√®tres\n",
    "\n",
    "L'analyse des courbes ci-dessus met en √©vidence trois compromis majeurs pour le choix du `min_support` :\n",
    "\n",
    "1.  **Explosion combinatoire (Quantit√©)** : Le nombre d'itemsets diminue de mani√®re exponentielle. Avec un support de **0.10**, on obtient pr√®s de 300 000 itemsets, ce qui est tr√®s co√ªteux en calcul et contient probablement beaucoup de \"bruit\". √Ä **0.15**, ce nombre chute √† environ 50 000, rendant l'analyse beaucoup plus rapide.\n",
    "\n",
    "2.  **Qualit√© des r√®gles (Lift & Confidence)** : On observe que la qualit√© moyenne des r√®gles diminue lorsque le support augmente. \n",
    "    *   Les associations tr√®s fr√©quentes (support √©lev√©) sont souvent g√©n√©riques (mots comme \"lyon\", \"france\"), d'o√π un *Lift* plus faible (~1.50).\n",
    "    *   Les associations plus rares (support faible) sont souvent plus fortes et sp√©cifiques (ex: \"basilique\" + \"fourviere\"), avec un *Lift* plus √©lev√© (~1.90).\n",
    "\n",
    "3.  **Couverture des clusters** : La couverture reste excellente (> 96%) jusqu'√† un support de **0.15**. Elle ne commence √† chuter significativement qu'apr√®s 0.20.\n",
    "\n",
    "**Conclusion** : \n",
    "Le point d'√©quilibre (\"le coude\") semble se situer autour de **0.15**.\n",
    "*   On √©limine l'explosion combinatoire (division par 6 du nombre d'itemsets).\n",
    "*   On maintient une excellente couverture (~96.5% des clusters sont labellis√©s).\n",
    "*   On conserve une qualit√© de r√®gles acceptable.\n",
    "\n",
    "Nous allons donc retenir **`min_support = 0.15`** pour la g√©n√©ration finale des r√®gles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4f6aa",
   "metadata": {},
   "source": [
    "### G√©n√©ration des r√®gles d'association\n",
    "\n",
    "On g√©n√®re les r√®gles avec une confiance minimale de 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.15, num_itemsets=len(transactions))\n",
    "rules = rules.sort_values('lift', ascending=False)\n",
    "\n",
    "print(f\"‚úì {len(rules)} r√®gles g√©n√©r√©es\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(100).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343045b6",
   "metadata": {},
   "source": [
    "### Top r√®gles par lift\n",
    "\n",
    "Le **lift** mesure la force de l'association (lift > 1 = association positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 r√®gles\n",
    "top_rules = rules.head(100).copy()\n",
    "\n",
    "# Formater pour l'affichage\n",
    "top_rules['rule'] = top_rules.apply(\n",
    "    lambda row: f\"{set(row['antecedents'])} ‚Üí {set(row['consequents'])}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(top_rules[['rule', 'support', 'confidence', 'lift']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4d21f",
   "metadata": {},
   "source": [
    "### Attribution des labels aux clusters\n",
    "\n",
    "Pour chaque cluster, on cherche l'itemset fr√©quent le plus repr√©sentatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa91422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un label par cluster bas√© sur les itemsets fr√©quents de longueur 2-3\n",
    "cluster_labels_assoc = {}\n",
    "\n",
    "for cluster_id in sorted(df['cluster_hdbscan'].unique()):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    \n",
    "    # Mots du cluster\n",
    "    cluster_words = set(transactions[cluster_id])\n",
    "    \n",
    "    # Chercher le meilleur itemset qui correspond\n",
    "    best_itemset = None\n",
    "    best_support = 0\n",
    "    \n",
    "    for _, row in frequent_itemsets[frequent_itemsets['length'] >= 2].iterrows():\n",
    "        itemset = row['itemsets']\n",
    "        if itemset.issubset(cluster_words) and row['support'] > best_support:\n",
    "            best_itemset = itemset\n",
    "            best_support = row['support']\n",
    "    \n",
    "    if best_itemset:\n",
    "        cluster_labels_assoc[cluster_id] = \" + \".join(sorted(best_itemset))\n",
    "    else:\n",
    "        cluster_labels_assoc[cluster_id] = \"unknown\"\n",
    "\n",
    "# Ajouter au DataFrame\n",
    "df['cluster_association'] = df['cluster_hdbscan'].map(cluster_labels_assoc).fillna(\"noise\")\n",
    "\n",
    "print(\"‚úì Labels d'association ajout√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb0b48",
   "metadata": {},
   "source": [
    "On visualise le r√©sultat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec941fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "sample_size = 30000\n",
    "sample = df.sample(n=min(sample_size, len(df)), random_state=0)\n",
    "\n",
    "m = folium.Map(\n",
    "    location=[df[\"lat\"].median(), df[\"long\"].median()],\n",
    "    zoom_start=12,\n",
    "    tiles=\"CartoDB positron\"\n",
    ")\n",
    "\n",
    "palette = [\n",
    "    \"red\", \"blue\", \"green\", \"purple\", \"orange\",\n",
    "    \"darkred\", \"lightred\", \"beige\", \"darkblue\",\n",
    "    \"darkgreen\", \"cadetblue\", \"darkpurple\",\n",
    "    \"pink\", \"lightblue\", \"lightgreen\",\n",
    "    \"gray\", \"black\", \"lightgray\"\n",
    "]\n",
    "\n",
    "for _, r in sample.iterrows():\n",
    "    cluster = r[\"cluster_hdbscan\"]\n",
    "    if cluster == -1:\n",
    "        continue\n",
    "    \n",
    "    color = palette[cluster % len(palette)]\n",
    "    label = r[\"cluster_association\"]\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[r[\"lat\"], r[\"long\"]],\n",
    "        radius=2,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.6,\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>Association:</b> {label}<br/>\n",
    "               <b>Cluster:</b> {cluster}<br/>\n",
    "               <a href=\"{r.get('url', '#')}\" target=\"_blank\">Open Flickr</a>\"\"\",\n",
    "            max_width=250\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "title_html = '''\n",
    "<div style=\"position: fixed; top: 10px; left: 50px; width: 400px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "     <b>M√©thode 3 : R√®gles d'association</b>\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "m.save(\"output/text_mining_association.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b7566",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
