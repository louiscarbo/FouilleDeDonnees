{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973df272",
   "metadata": {},
   "source": [
    "# Text Mining \n",
    "\n",
    "## 1. Pr√©processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"flickr_data_clustered.parquet\")\n",
    "print(f\"Donn√©es charg√©es : {len(df)} photos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71074a77",
   "metadata": {},
   "source": [
    "### D√©finition des stopwords\n",
    "\n",
    "On d√©finit manuellement une liste de stopwords, contenant les mots non-pertinents pour identifier les lieux touristiques de Lyon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21580870",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {\n",
    "    \"lyon\", \"france\", \"img\", \"jpg\", \"uploaded\", \"europe\", \"square\", \"iphone\", \"instagram\",\n",
    "    \"franca\", \"rhonealpes\", \"geotagged\", \"janvier\", \"fevrier\", \"mars\", \"avril\", \"mai\", \"juin\",\n",
    "    \"juillet\", \"aout\", \"septembre\", \"octobre\", \"novembre\", \"decembre\", \"interieur\", \"live\",\n",
    "    \"des\", \"squareformat\", \"dsc\", \"iphoneography\", \"art\", \"instagramapp\", \"foursquare\",\n",
    "    \"venue\", \"japan\", \"flickrmobile\", \"french\", \"francia\", \"dscf\", \"frankrijk\", \"frankreich\",\n",
    "    \"flickriosapp\", \"touch\"\n",
    "}\n",
    "\n",
    "print(f\"Nombre total de stopwords : {len(stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da970c1",
   "metadata": {},
   "source": [
    "### Fonction de nettoyage du texte\n",
    "\n",
    "On cr√©e une fonction qui :\n",
    "1. Concat√®ne title et tags\n",
    "2. Met en minuscules\n",
    "3. Supprime les accents (√© ‚Üí e, √ß ‚Üí c)\n",
    "4. Supprime la ponctuation et caract√®res sp√©ciaux\n",
    "5. Filtre les stopwords et mots trop courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(title, tags):\n",
    "    \"\"\"\n",
    "    Nettoie et combine title et tags en une liste de mots pertinents.\n",
    "    \n",
    "    Args:\n",
    "        title: Titre de la photo (str ou NaN)\n",
    "        tags: Tags de la photo (str ou NaN)\n",
    "    \n",
    "    Returns:\n",
    "        Liste de mots nettoy√©s\n",
    "    \"\"\"\n",
    "    # Combiner title et tags\n",
    "    text = \"\"\n",
    "    if isinstance(title, str):\n",
    "        text += title + \" \"\n",
    "    if isinstance(tags, str):\n",
    "        text += tags\n",
    "    \n",
    "    if not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # 1. Minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Supprimer accents et caract√®res sp√©ciaux Unicode\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "    \n",
    "    # 3. Ne garder que lettres et espaces (supprime [, ], {, }, etc.)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # 4. Split et filtrer\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 2]\n",
    "    \n",
    "    # 5. Supprimer les doublons tout en pr√©servant l'ordre\n",
    "    unique_words = list(dict.fromkeys(words))\n",
    "    \n",
    "    return unique_words\n",
    "\n",
    "# Test de la fonction\n",
    "test_title = \"[Lyon] Basilique de Fourvi√®re - √ât√© 2024\"\n",
    "test_tags = \"france, architecture, √©glise, photo\"\n",
    "print(\"Test de nettoyage :\")\n",
    "print(f\"Input : '{test_title}' + '{test_tags}'\")\n",
    "print(f\"Output : {clean_text(test_title, test_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a7f3a",
   "metadata": {},
   "source": [
    "### Application du nettoyage\n",
    "\n",
    "On applique la fonction de nettoyage sur toutes les photos pour cr√©er une colonne unique `cleaned_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer le nettoyage sur title + tags\n",
    "df['cleaned_text'] = df.apply(\n",
    "    lambda row: clean_text(row['title'], row['tags']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Statistiques sur le nettoyage\n",
    "total_photos = len(df)\n",
    "photos_avec_mots = (df['cleaned_text'].str.len() > 0).sum()\n",
    "photos_sans_mots = total_photos - photos_avec_mots\n",
    "\n",
    "print(f\"‚úì Nettoyage termin√©\")\n",
    "print(f\"  - Photos avec mots : {photos_avec_mots} ({photos_avec_mots/total_photos*100:.1f}%)\")\n",
    "print(f\"  - Photos sans mots : {photos_sans_mots} ({photos_sans_mots/total_photos*100:.1f}%)\")\n",
    "print(f\"  - Nombre moyen de mots/photo : {df['cleaned_text'].str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d848f",
   "metadata": {},
   "source": [
    "### V√©rification du r√©sultat\n",
    "\n",
    "On affiche quelques exemples pour v√©rifier que le nettoyage fonctionne correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c706af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher quelques exemples\n",
    "sample = df[df['cleaned_text'].str.len() > 0].sample(n=10, random_state=42)\n",
    "\n",
    "print(\"Exemples de textes nettoy√©s :\\n\")\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"Title original : {row['title'][:80]}...\")\n",
    "    print(f\"Tags originaux : {row['tags'][:80] if isinstance(row['tags'], str) else 'N/A'}...\")\n",
    "    print(f\"Mots nettoy√©s  : {row['cleaned_text'][:10]}\")  # 10 premiers mots\n",
    "    print(f\"Cluster        : {row['cluster_hdbscan']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8ca8e",
   "metadata": {},
   "source": [
    "## 2. M√©thode 1 : Mot le plus fr√©quent par cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021bbe3",
   "metadata": {},
   "source": [
    "### Principe\n",
    "\n",
    "Pour chaque cluster, on identifie le mot le plus fr√©quent dans les textes nettoy√©s (titles + tags).\n",
    "\n",
    "**Hypoth√®se** : Le mot dominant repr√©sente le lieu ou le monument principal du cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057c0c3",
   "metadata": {},
   "source": [
    "### Fonction de calcul des mots dominants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_cluster_keywords(df, cluster_col='cluster_hdbscan', text_col='cleaned_text', top_k=1):\n",
    "    \"\"\"\n",
    "    Trouve le(s) mot(s) le(s) plus fr√©quent(s) pour chaque cluster.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame avec colonnes cluster et texte nettoy√©\n",
    "        cluster_col: Nom de la colonne contenant les labels de cluster\n",
    "        text_col: Nom de la colonne contenant les listes de mots nettoy√©s\n",
    "        top_k: Nombre de mots √† retourner par cluster\n",
    "    \n",
    "    Returns:\n",
    "        dict {cluster_id: mot_dominant}\n",
    "    \"\"\"\n",
    "    cluster_keywords = {}\n",
    "    \n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        if cluster_id == -1:\n",
    "            continue  # Ignorer le bruit\n",
    "        \n",
    "        # R√©cup√©rer tous les mots du cluster\n",
    "        cluster_data = df[df[cluster_col] == cluster_id]\n",
    "        all_words = []\n",
    "        \n",
    "        for words_list in cluster_data[text_col].dropna():\n",
    "            all_words.extend(words_list)\n",
    "        \n",
    "        # Si pas de mots, mettre \"unknown\"\n",
    "        if not all_words:\n",
    "            cluster_keywords[cluster_id] = \"unknown\"\n",
    "            continue\n",
    "        \n",
    "        # Compter les occurrences\n",
    "        word_counts = Counter(all_words)\n",
    "        \n",
    "        # Prendre le mot le plus fr√©quent\n",
    "        top_word = word_counts.most_common(top_k)[0][0]\n",
    "        cluster_keywords[cluster_id] = top_word\n",
    "    \n",
    "    return cluster_keywords\n",
    "\n",
    "# Calculer les mots-cl√©s dominants\n",
    "cluster_keywords = find_cluster_keywords(df)\n",
    "\n",
    "# Ajouter au DataFrame\n",
    "df['cluster_keyword'] = df['cluster_hdbscan'].map(cluster_keywords).fillna(\"noise\")\n",
    "\n",
    "print(f\"‚úì Mots-cl√©s calcul√©s pour {len(cluster_keywords)} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736abcd",
   "metadata": {},
   "source": [
    "### Statistiques et aper√ßu des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le r√©sum√© des clusters\n",
    "cluster_summary = df[df['cluster_hdbscan'] != -1].groupby('cluster_hdbscan').agg({\n",
    "    'cluster_keyword': 'first',\n",
    "    'id': 'count'\n",
    "}).rename(columns={'id': 'nb_photos'}).sort_values('nb_photos', ascending=False)\n",
    "\n",
    "n_clusters = len(cluster_summary)\n",
    "\n",
    "# Compter les mots-cl√©s par CLUSTER (pas par photo)\n",
    "keyword_cluster_counts = cluster_summary['cluster_keyword'].value_counts()\n",
    "n_unique_keywords = (keyword_cluster_counts == 1).sum()\n",
    "n_duplicate_keywords = len(keyword_cluster_counts[keyword_cluster_counts > 1])\n",
    "\n",
    "print(f\"üîç Qualit√© des mots-cl√©s:\")\n",
    "print(f\"  - Mots-cl√©s uniques: {n_unique_keywords}/{n_clusters} ({n_unique_keywords/n_clusters*100:.1f}%)\")\n",
    "print(f\"  - Mots-cl√©s dupliqu√©s: {n_duplicate_keywords} (partag√©s par plusieurs clusters)\")\n",
    "print(f\"  ‚ö†Ô∏è  Ambigu√Øt√©: {n_clusters - n_unique_keywords} clusters partagent un m√™me mot-cl√©\")\n",
    "\n",
    "# Top des mots-cl√©s les plus fr√©quents (candidats stopwords)\n",
    "print(f\"\\nüö´ Mots-cl√©s les plus fr√©quents (candidats pour stopwords):\")\n",
    "print(f\"    Ces mots-cl√©s dominent plusieurs clusters, ils sont donc peu distinctifs\\n\")\n",
    "for keyword, count in keyword_cluster_counts.head(20).items():\n",
    "    print(f\"    '{keyword}' : {count} clusters\")\n",
    "\n",
    "print(f\"\\nüìç Top 20 clusters par taille :\\n\")\n",
    "print(cluster_summary.head(20).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb3488",
   "metadata": {},
   "source": [
    "### Visualisation sur carte interactive\n",
    "\n",
    "On affiche les clusters sur une carte avec leurs mots-cl√©s dominants pour identifier visuellement les zones d'int√©r√™t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c77620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# √âchantillonner pour la performance\n",
    "sample_size = 30000\n",
    "sample = df.sample(n=min(sample_size, len(df)), random_state=0)\n",
    "\n",
    "# Cr√©er la carte\n",
    "m = folium.Map(\n",
    "    location=[df[\"lat\"].median(), df[\"long\"].median()],\n",
    "    zoom_start=12,\n",
    "    tiles=\"CartoDB positron\"\n",
    ")\n",
    "\n",
    "# Palette de couleurs\n",
    "palette = [\n",
    "    \"red\", \"blue\", \"green\", \"purple\", \"orange\",\n",
    "    \"darkred\", \"lightred\", \"beige\", \"darkblue\",\n",
    "    \"darkgreen\", \"cadetblue\", \"darkpurple\",\n",
    "    \"pink\", \"lightblue\", \"lightgreen\",\n",
    "    \"gray\", \"black\", \"lightgray\"\n",
    "]\n",
    "\n",
    "# Ajouter les points\n",
    "for _, r in sample.iterrows():\n",
    "    cluster = r[\"cluster_hdbscan\"]\n",
    "    \n",
    "    if cluster == -1:\n",
    "        continue\n",
    "    else:\n",
    "        color = palette[cluster % len(palette)]\n",
    "        keyword = r[\"cluster_keyword\"]\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[r[\"lat\"], r[\"long\"]],\n",
    "        radius=2,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.6,\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>Keyword:</b> {keyword}<br/>\n",
    "               <b>Cluster:</b> {cluster}<br/>\n",
    "               <a href=\"{r.get('url', '#')}\" target=\"_blank\">Open Flickr</a>\"\"\",\n",
    "            max_width=250\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "# Titre sur la carte\n",
    "title_html = '''\n",
    "<div style=\"position: fixed; top: 10px; left: 50px; width: 400px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "     <b>M√©thode 1 : Mot le plus fr√©quent par cluster</b>\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Sauvegarder\n",
    "m.save(\"output/text_mining_method1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edaaa74",
   "metadata": {},
   "source": [
    "### Analyse des r√©sultats\n",
    "\n",
    "**Observations** :\n",
    "\n",
    "En explorant la carte, on peut identifier les mots-cl√©s dominants pour chaque zone touristique. Certains mots sont pertinents (ex: \"croixrousse\", \"bellecour\", \"jacobins\"), d'autres sont trop g√©n√©riques ou non-informatifs.\n",
    "\n",
    "**Limites de cette m√©thode** :\n",
    "\n",
    "1. **Sensibilit√© aux stopwords** : La qualit√© d√©pend fortement de la liste de stopwords. Des mots non-pertinents peuvent dominer si on ne les filtre pas manuellement, ce qui est fastidieux et propice aux erreurs.\n",
    "\n",
    "2. **Mot unique par cluster** : Un seul mot ne suffit pas toujours √† caract√©riser un lieu (ex: \"Part Dieu\" ‚Üí seulement \"part\" ou \"dieu\").\n",
    "\n",
    "3. **Approche it√©rative** : N√©cessite d'inspecter visuellement les r√©sultats et d'ajuster manuellement les stopwords pour am√©liorer la qualit√©.\n",
    "\n",
    "4. **Pas de pond√©ration contextuelle** : Tous les mots ont le m√™me poids, qu'ils soient sp√©cifiques (ex: \"fourviere\") ou g√©n√©riques (ex: \"rue\", \"statue\").\n",
    "\n",
    "**Am√©lioration possible** : Utiliser TF-IDF pour pond√©rer l'importance des mots en fonction de leur sp√©cificit√© √† chaque cluster, plut√¥t que simplement compter les fr√©quences brutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914ba61",
   "metadata": {},
   "source": [
    "## 3. M√©thode 2 : TF-IDF\n",
    "\n",
    "Pour pallier les limites de la simple fr√©quence, on utilise **TF-IDF** (Term Frequency - Inverse Document Frequency).\n",
    "\n",
    "**Principe** : TF-IDF donne un score √©lev√© aux mots :\n",
    "- Fr√©quents dans un cluster sp√©cifique (TF)\n",
    "- Rares dans les autres clusters (IDF)\n",
    "\n",
    "Cela permet de capturer les mots **distinctifs** plut√¥t que simplement fr√©quents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a876c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combiner les listes de mots nettoy√©s en un seul texte par photo\n",
    "df['combined_text'] = df.apply(\n",
    "    lambda row: ' '.join(row['cleaned_title_stopwords'] + row['cleaned_tags_stopwords']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# V√©rifier le r√©sultat\n",
    "df[['cleaned_title_stopwords', 'cleaned_tags_stopwords', 'combined_text', 'cluster_hdbscan']].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d1dcf",
   "metadata": {},
   "source": [
    "Pour chaque cluster, on combine les textes de toutes les photos appartenant √† ce cluster en un seul document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7523d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_documents = df.groupby('cluster_hdbscan')['combined_text'].apply(\n",
    "    lambda texts: ' '.join(texts)\n",
    ").reset_index()\n",
    "\n",
    "cluster_documents.columns = ['cluster', 'document']\n",
    "\n",
    "# Afficher les premiers clusters avec leur taille de texte\n",
    "cluster_documents['text_length'] = cluster_documents['document'].str.len()\n",
    "cluster_documents[['cluster', 'text_length']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6b7a6a",
   "metadata": {},
   "source": [
    "On calcule ensuite le TF-IDF pour chaque mot dans chaque document de cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Cr√©er le vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # Garder les 5000 mots les plus importants\n",
    "    min_df=2,           # Le mot doit appara√Ætre dans au moins 2 clusters\n",
    "    max_df=0.8          # Le mot ne doit pas √™tre dans plus de 80% des clusters\n",
    ")\n",
    "\n",
    "# Calculer la matrice TF-IDF\n",
    "tfidf_matrix = tfidf.fit_transform(cluster_documents['document'])\n",
    "\n",
    "# Voir la forme de la matrice\n",
    "print(f\"Matrice TF-IDF : {tfidf_matrix.shape}\")\n",
    "print(f\"(nombre_clusters √ó nombre_termes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dcaabf",
   "metadata": {},
   "source": [
    "On extrait ensuite le top k mots avec les scores TF-IDF les plus √©lev√©s pour chaque cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3600c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# R√©cup√©rer les noms des termes\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Fonction pour extraire les top N termes d'un cluster\n",
    "def get_top_terms(cluster_idx, n=10):\n",
    "    # R√©cup√©rer les scores TF-IDF pour ce cluster\n",
    "    scores = tfidf_matrix[cluster_idx].toarray().flatten()\n",
    "    \n",
    "    # Obtenir les indices des top termes\n",
    "    top_indices = scores.argsort()[-n:][::-1]\n",
    "    \n",
    "    # Cr√©er un dataframe\n",
    "    top_terms = pd.DataFrame({\n",
    "        'term': feature_names[top_indices],\n",
    "        'tfidf_score': scores[top_indices]\n",
    "    })\n",
    "    \n",
    "    return top_terms\n",
    "\n",
    "# Afficher les top termes du cluster 0\n",
    "print(\"Top 10 termes du cluster 0 :\")\n",
    "get_top_terms(0, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab86105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les top 10 termes des 5 premiers clusters\n",
    "for i in range(min(5, len(cluster_documents))):\n",
    "    cluster_id = cluster_documents.iloc[i]['cluster']\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER {cluster_id} - Top 10 termes :\")\n",
    "    print(f\"{'='*60}\")\n",
    "    display(get_top_terms(i, n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder avec la colonne cluster_name\n",
    "df.to_parquet(\"flickr_data_clusters_mined.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
