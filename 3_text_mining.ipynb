{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973df272",
   "metadata": {},
   "source": [
    "# Text Mining \n",
    "\n",
    "## 1. Pr√©processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"data_clustered.parquet\")\n",
    "print(f\"Donn√©es charg√©es : {len(df)} photos\")\n",
    "print(f\"Nombre de clusters : {df['cluster_hdbscan'].nunique() - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71074a77",
   "metadata": {},
   "source": [
    "### D√©finition des stopwords\n",
    "\n",
    "On d√©finit manuellement une liste de stopwords, contenant les mots non-pertinents pour identifier les lieux touristiques de Lyon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21580870",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_from_method1 = {\n",
    "    \"lyon\", \"france\", \"img\", \"jpg\", \"uploaded\", \"europe\", \"square\", \"iphone\", \"instagram\",\n",
    "    \"franca\", \"rhonealpes\", \"geotagged\", \"janvier\", \"fevrier\", \"mars\", \"avril\", \"mai\", \"juin\",\n",
    "    \"juillet\", \"aout\", \"septembre\", \"octobre\", \"novembre\", \"decembre\", \"interieur\", \"live\",\n",
    "    \"des\", \"squareformat\", \"dsc\", \"iphoneography\", \"art\", \"instagramapp\", \"foursquare\",\n",
    "    \"venue\", \"japan\", \"flickrmobile\", \"french\", \"francia\", \"dscf\", \"frankrijk\", \"frankreich\",\n",
    "    \"flickriosapp\", \"touch\"\n",
    "}\n",
    "\n",
    "stopwords_from_method2 = {\n",
    "    \"ngc\", \"paysage\", \"landscape\", \"upload\", \"ios\", \"flickr\", \"mobile\", \"app\", \"filter\",\n",
    "    \"blackandwhite\", \"black\", \"white\", \"phone\", \"noir\", \"blanc\", \"the\", \"nos\",\n",
    "    \"canon\", \"mmf\", \"monochrome\", \"alpes\", \"rhones\", \"photo\", \"urban\", \"photos\", \"nikon\",\n",
    "    \"night\", \"auvergne\", \"urbain\", \"photography\", \"town\", \"city\", \"les\", \"street\",\n",
    "    \"architecture\", \"ville\", \"reich\", \"frankreich\", \"sky\", \"metropolisoflyon\",\n",
    "    \"burgundy\", \"bourgogne\", \"auvergnerhonealpes\", \"geo\", \"and\", \"lat\", \"lon\", \"lione\",\n",
    "    \"eme\", \"rhone\", \"nuit\", \"noiretblanc\", \"light\", \"people\"\n",
    "}\n",
    "\n",
    "# Unifier les 2 ensembles\n",
    "stopwords = stopwords_from_method1.union(stopwords_from_method2)\n",
    "\n",
    "print(f\"Nombre total de stopwords : {len(stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179738b",
   "metadata": {},
   "source": [
    "### Tokenization : Mis de c√¥t√©, r√©sultats peu probants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493c76e",
   "metadata": {},
   "source": [
    "La tokenization est mise de c√¥t√© car les r√©sultats obtenus ne sont pas pertinents : d'apr√®s nos tests, nous n'avons pas trouv√© de librairie permettant de segmenter correctement les \"hashtags\" en mots significatifs. Par exemple, \"basiliquedefourviere\" n'est pas segment√© en \"basilique de fourviere\", ou bien l'introduction de segmentation d√©grade le reste des tags d√©j√† correctement segment√©s.\n",
    "\n",
    "La partie \"segmentation\" est donc √† ignorer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae91eb",
   "metadata": {},
   "source": [
    "On applique une tokenization afin de mieux traiter les hashtags, par exemple : \"fetedeslumieres\" devient \"fete\", \"des\", et \"lumieres\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c669915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import subprocess\n",
    "# import unicodedata\n",
    "\n",
    "# # 1. Installation de ekphrasis ET pyspellchecker\n",
    "# try:\n",
    "#     from ekphrasis.classes.segmenter import Segmenter\n",
    "#     from spellchecker import SpellChecker\n",
    "# except ImportError:\n",
    "#     print(\"Installation des librairies n√©cessaires...\")\n",
    "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ekphrasis\", \"pyspellchecker\"])\n",
    "#     from ekphrasis.classes.segmenter import Segmenter\n",
    "#     from spellchecker import SpellChecker\n",
    "\n",
    "# # 2. Chargement \n",
    "# try:\n",
    "#     # On utilise le corpus 'twitter' qui marche pour les hashtags\n",
    "#     seg_tw = Segmenter(corpus=\"twitter\")\n",
    "#     spell_fr = SpellChecker(language='fr') \n",
    "#     print(\"‚úì Segmenter Ekphrasis charg√©\")\n",
    "#     print(\"‚úì Dictionnaire Fran√ßais charg√©\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Erreur au chargement: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da970c1",
   "metadata": {},
   "source": [
    "### Fonction de nettoyage du texte\n",
    "\n",
    "On cr√©e une fonction qui :\n",
    "1. Concat√®ne title et tags\n",
    "2. Met en minuscules\n",
    "3. Supprime les accents (√© ‚Üí e, √ß ‚Üí c)\n",
    "4. Supprime la ponctuation et caract√®res sp√©ciaux\n",
    "5. Filtre les stopwords et mots trop courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(title, tags):\n",
    "    # Combiner title et tags\n",
    "    text = \"\"\n",
    "    if isinstance(title, str):\n",
    "        text += title + \" \"\n",
    "    if isinstance(tags, str):\n",
    "        text += tags\n",
    "    \n",
    "    if not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # 1. Minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Supprimer accents\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "    \n",
    "    # 3. Ne garder que lettres et espaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # 4. Split initial\n",
    "    words = text.split()\n",
    "    \n",
    "    # 5. Segmentation Intelligente avec Ekphrasis\n",
    "    # LA SEGMENTATION EST MISE DE C√ñT√â (voir note plus haut)\n",
    "    # expanded_words = []\n",
    "    # for w in words:\n",
    "    #     # STRAT√âGIE :\n",
    "    #     # 1. Si c'est un mot fran√ßais valide -> On garde tel quel\n",
    "    #     # 2. Sinon -> On demande √† Ekphrasis de le segmenter\n",
    "        \n",
    "    #     if (w in spell_fr) or (len(w) < 4):\n",
    "    #         expanded_words.append(w)\n",
    "    #     else:\n",
    "    #         try:\n",
    "    #             # ekphrasis renvoie une string avec des espaces \"mot1 mot2\"\n",
    "    #             segmented = seg_tw.segment(w)\n",
    "    #             expanded_words.extend(segmented.split())\n",
    "    #         except:\n",
    "    #             expanded_words.append(w)\n",
    "    \n",
    "    # 6. Filtrer stopwords et mots courts (re-v√©rification apr√®s segmentation)\n",
    "    expanded_words = words  # Sans segmentation\n",
    "    final_words = [w for w in expanded_words if w not in stopwords and len(w) > 2]\n",
    "    \n",
    "    # 7. Uniques\n",
    "    unique_words = list(dict.fromkeys(final_words))\n",
    "    \n",
    "    return unique_words\n",
    "\n",
    "# Test de la fonction de nettoyage\n",
    "print(clean_text(\"Visite de la Basilique de Fourvi√®re en √©t√© !\", \"basiliquedefourviere lyon france summer travel\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a7f3a",
   "metadata": {},
   "source": [
    "### Application du nettoyage\n",
    "\n",
    "On applique la fonction de nettoyage sur toutes les photos pour cr√©er une colonne unique `cleaned_text`.\n",
    "\n",
    "**Note** : Cette √©tape peut prendre quelques minutes avec `wordsegment`. Une barre de progression s'affichera pour suivre l'avancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "output_file = \"data_clustered_tokenized.parquet\"\n",
    "\n",
    "# LA SEGMENTATION EST MISE DE C√ñT√â (voir note plus haut)\n",
    "# # On √©vite de refaire la tokenization (qui prend du temps) si c'est d√©j√† fait\n",
    "# if os.path.exists(output_file):\n",
    "#     print(f\"üîÑ Chargement des donn√©es tokenis√©es depuis '{output_file}'...\")\n",
    "#     df = pd.read_parquet(output_file)\n",
    "#     print(\"‚úÖ Donn√©es charg√©es avec succ√®s ! (Traitement pass√©)\")\n",
    "# else:\n",
    "#     print(f\"üöÄ D√©marrage du traitement (Nettoyage + Tokenisation)...\")\n",
    "#     tqdm.pandas(desc=\"Tokenisation en cours\")\n",
    "    \n",
    "#     # Appliquer le nettoyage sur title + tags avec barre de progression\n",
    "#     df['cleaned_text'] = df.progress_apply(\n",
    "#         lambda row: clean_text(row['title'], row['tags']), \n",
    "#         axis=1\n",
    "#     )\n",
    "    \n",
    "#     print(f\"üíæ Sauvegarde du r√©sultat dans '{output_file}'...\")\n",
    "#     df.to_parquet(output_file)\n",
    "#     print(\"‚úÖ Traitement termin√© et sauvegard√© !\")\n",
    "\n",
    "df['cleaned_text'] = df.progress_apply(\n",
    "    lambda row: clean_text(row['title'], row['tags']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Statistiques sur le nettoyage\n",
    "total_photos = len(df)\n",
    "photos_avec_mots = (df['cleaned_text'].str.len() > 0).sum()\n",
    "photos_sans_mots = total_photos - photos_avec_mots\n",
    "\n",
    "print(f\"\\nüìä Statistiques :\")\n",
    "print(f\"  - Photos avec mots : {photos_avec_mots} ({photos_avec_mots/total_photos*100:.1f}%)\")\n",
    "print(f\"  - Photos sans mots : {photos_sans_mots} ({photos_sans_mots/total_photos*100:.1f}%)\")\n",
    "print(f\"  - Nombre moyen de mots/photo : {df['cleaned_text'].str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d848f",
   "metadata": {},
   "source": [
    "### V√©rification du r√©sultat\n",
    "\n",
    "On affiche quelques exemples pour v√©rifier que le nettoyage fonctionne correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c706af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher quelques exemples\n",
    "sample = df[df['cleaned_text'].str.len() > 0].sample(n=10, random_state=42)\n",
    "\n",
    "print(\"Exemples de textes nettoy√©s :\\n\")\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"Title original : {row['title'][:80]}...\")\n",
    "    print(f\"Tags originaux : {row['tags'][:80] if isinstance(row['tags'], str) else 'N/A'}...\")\n",
    "    print(f\"Mots nettoy√©s  : {row['cleaned_text'][:10]}\")  # 10 premiers mots\n",
    "    print(f\"Cluster        : {row['cluster_hdbscan']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8ca8e",
   "metadata": {},
   "source": [
    "## 2. M√©thode 1 : Mot le plus fr√©quent par cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021bbe3",
   "metadata": {},
   "source": [
    "### Principe\n",
    "\n",
    "Pour chaque cluster, on identifie le mot le plus fr√©quent dans les textes nettoy√©s (titles + tags).\n",
    "\n",
    "**Hypoth√®se** : Le mot dominant repr√©sente le lieu ou le monument principal du cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057c0c3",
   "metadata": {},
   "source": [
    "### Fonction de calcul des mots dominants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_cluster_keywords(df, cluster_col='cluster_hdbscan', text_col='cleaned_text', top_k=1):\n",
    "    \"\"\"\n",
    "    Trouve le(s) mot(s) le(s) plus fr√©quent(s) pour chaque cluster.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame avec colonnes cluster et texte nettoy√©\n",
    "        cluster_col: Nom de la colonne contenant les labels de cluster\n",
    "        text_col: Nom de la colonne contenant les listes de mots nettoy√©s\n",
    "        top_k: Nombre de mots √† retourner par cluster\n",
    "    \n",
    "    Returns:\n",
    "        dict {cluster_id: mot_dominant}\n",
    "    \"\"\"\n",
    "    cluster_keywords = {}\n",
    "    \n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        if cluster_id == -1:\n",
    "            continue  # Ignorer le bruit\n",
    "        \n",
    "        # R√©cup√©rer tous les mots du cluster\n",
    "        cluster_data = df[df[cluster_col] == cluster_id]\n",
    "        all_words = []\n",
    "        \n",
    "        for words_list in cluster_data[text_col].dropna():\n",
    "            all_words.extend(words_list)\n",
    "        \n",
    "        # Si pas de mots, mettre \"unknown\"\n",
    "        if not all_words:\n",
    "            cluster_keywords[cluster_id] = \"unknown\"\n",
    "            continue\n",
    "        \n",
    "        # Compter les occurrences\n",
    "        word_counts = Counter(all_words)\n",
    "        \n",
    "        # Prendre le mot le plus fr√©quent\n",
    "        top_word = word_counts.most_common(top_k)[0][0]\n",
    "        cluster_keywords[cluster_id] = top_word\n",
    "    \n",
    "    return cluster_keywords\n",
    "\n",
    "# Calculer les mots-cl√©s dominants\n",
    "cluster_keywords = find_cluster_keywords(df)\n",
    "\n",
    "# Ajouter au DataFrame\n",
    "df['cluster_keyword'] = df['cluster_hdbscan'].map(cluster_keywords).fillna(\"noise\")\n",
    "\n",
    "print(f\"‚úì Mots-cl√©s calcul√©s pour {len(cluster_keywords)} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736abcd",
   "metadata": {},
   "source": [
    "### Statistiques et aper√ßu des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le r√©sum√© des clusters\n",
    "cluster_summary = df[df['cluster_hdbscan'] != -1].groupby('cluster_hdbscan').agg({\n",
    "    'cluster_keyword': 'first',\n",
    "    'id': 'count'\n",
    "}).rename(columns={'id': 'nb_photos'}).sort_values('nb_photos', ascending=False)\n",
    "\n",
    "n_clusters = len(cluster_summary)\n",
    "\n",
    "# Compter les mots-cl√©s par CLUSTER (pas par photo)\n",
    "keyword_cluster_counts = cluster_summary['cluster_keyword'].value_counts()\n",
    "n_unique_keywords = (keyword_cluster_counts == 1).sum()\n",
    "n_duplicate_keywords = len(keyword_cluster_counts[keyword_cluster_counts > 1])\n",
    "\n",
    "print(f\"üîç Qualit√© des mots-cl√©s:\")\n",
    "print(f\"  - Mots-cl√©s uniques: {n_unique_keywords}/{n_clusters} ({n_unique_keywords/n_clusters*100:.1f}%)\")\n",
    "print(f\"  - Mots-cl√©s dupliqu√©s: {n_duplicate_keywords} (partag√©s par plusieurs clusters)\")\n",
    "print(f\"  ‚ö†Ô∏è  Ambigu√Øt√©: {n_clusters - n_unique_keywords} clusters partagent un m√™me mot-cl√©\")\n",
    "\n",
    "# Top des mots-cl√©s les plus fr√©quents (candidats stopwords)\n",
    "print(f\"\\nüö´ Mots-cl√©s les plus fr√©quents (candidats pour stopwords):\")\n",
    "print(f\"    Ces mots-cl√©s dominent plusieurs clusters, ils sont donc peu distinctifs\\n\")\n",
    "for keyword, count in keyword_cluster_counts.head(20).items():\n",
    "    print(f\"    '{keyword}' : {count} clusters\")\n",
    "\n",
    "print(f\"\\nüìç Top 20 clusters par taille :\\n\")\n",
    "print(cluster_summary.head(20).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb3488",
   "metadata": {},
   "source": [
    "### Visualisation sur carte interactive\n",
    "\n",
    "On affiche les clusters sur une carte avec leurs mots-cl√©s dominants pour identifier visuellement les zones d'int√©r√™t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c77620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# √âchantillonner pour la performance\n",
    "sample_size = 30000\n",
    "sample = df.sample(n=min(sample_size, len(df)), random_state=0)\n",
    "\n",
    "# Cr√©er la carte\n",
    "m = folium.Map(\n",
    "    location=[df[\"lat\"].median(), df[\"long\"].median()],\n",
    "    zoom_start=12,\n",
    "    tiles=\"CartoDB positron\"\n",
    ")\n",
    "\n",
    "# Palette de couleurs\n",
    "palette = [\n",
    "    \"red\", \"blue\", \"green\", \"purple\", \"orange\",\n",
    "    \"darkred\", \"lightred\", \"beige\", \"darkblue\",\n",
    "    \"darkgreen\", \"cadetblue\", \"darkpurple\",\n",
    "    \"pink\", \"lightblue\", \"lightgreen\",\n",
    "    \"gray\", \"black\", \"lightgray\"\n",
    "]\n",
    "\n",
    "# Ajouter les points\n",
    "for _, r in sample.iterrows():\n",
    "    cluster = r[\"cluster_hdbscan\"]\n",
    "    \n",
    "    if cluster == -1:\n",
    "        continue\n",
    "    else:\n",
    "        color = palette[cluster % len(palette)]\n",
    "        keyword = r[\"cluster_keyword\"]\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[r[\"lat\"], r[\"long\"]],\n",
    "        radius=2,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.6,\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>Keyword:</b> {keyword}<br/>\n",
    "               <b>Cluster:</b> {cluster}<br/>\n",
    "               <a href=\"{r.get('url', '#')}\" target=\"_blank\">Open Flickr</a>\"\"\",\n",
    "            max_width=250\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "# Titre sur la carte\n",
    "title_html = '''\n",
    "<div style=\"position: fixed; top: 10px; left: 50px; width: 400px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "     <b>M√©thode 1 : Mot le plus fr√©quent par cluster</b>\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Sauvegarder\n",
    "m.save(\"output/text_mining_method1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edaaa74",
   "metadata": {},
   "source": [
    "### Analyse des r√©sultats\n",
    "\n",
    "**Observations** :\n",
    "\n",
    "En explorant la carte, on peut identifier les mots-cl√©s dominants pour chaque zone touristique. Certains mots sont pertinents (ex: \"croixrousse\", \"bellecour\", \"jacobins\"), d'autres sont trop g√©n√©riques ou non-informatifs.\n",
    "\n",
    "**Limites de cette m√©thode** :\n",
    "\n",
    "1. **Sensibilit√© aux stopwords** : La qualit√© d√©pend fortement de la liste de stopwords. Des mots non-pertinents peuvent dominer si on ne les filtre pas manuellement, ce qui est fastidieux et propice aux erreurs.\n",
    "\n",
    "2. **Mot unique par cluster** : Un seul mot ne suffit pas toujours √† caract√©riser un lieu (ex: \"Part Dieu\" ‚Üí seulement \"part\" ou \"dieu\").\n",
    "\n",
    "3. **Approche it√©rative** : N√©cessite d'inspecter visuellement les r√©sultats et d'ajuster manuellement les stopwords pour am√©liorer la qualit√©.\n",
    "\n",
    "4. **Pas de pond√©ration contextuelle** : Tous les mots ont le m√™me poids, qu'ils soient sp√©cifiques (ex: \"fourviere\") ou g√©n√©riques (ex: \"rue\", \"statue\").\n",
    "\n",
    "**Am√©lioration possible** : Utiliser TF-IDF pour pond√©rer l'importance des mots en fonction de leur sp√©cificit√© √† chaque cluster, plut√¥t que simplement compter les fr√©quences brutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914ba61",
   "metadata": {},
   "source": [
    "## 3. M√©thode 2 : TF-IDF\n",
    "\n",
    "Pour pallier les limites de la simple fr√©quence, on utilise **TF-IDF** (Term Frequency - Inverse Document Frequency).\n",
    "\n",
    "**Principe** : TF-IDF donne un score √©lev√© aux mots :\n",
    "- Fr√©quents dans un cluster sp√©cifique (TF)\n",
    "- Rares dans les autres clusters (IDF)\n",
    "\n",
    "Cela permet de capturer les mots **distinctifs** plut√¥t que simplement fr√©quents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7d76c",
   "metadata": {},
   "source": [
    "### Pr√©paration des textes\n",
    "\n",
    "On transforme chaque photo en un texte, puis on concat√®ne toutes les photos d‚Äôun cluster en un seul document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Texte par photo\n",
    "df['combined_text'] = df['cleaned_text'].apply(lambda words: ' '.join(words))\n",
    "\n",
    "# 2) Document par cluster\n",
    "cluster_documents = df[df['cluster_hdbscan'] != -1].groupby('cluster_hdbscan')['combined_text'] \\\n",
    "    .apply(lambda texts: ' '.join(texts)) \\\n",
    "    .reset_index()\n",
    "\n",
    "cluster_documents.columns = ['cluster', 'document']\n",
    "cluster_documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f044769",
   "metadata": {},
   "source": [
    "On regarde le nombre de clusters et la taille moyenne des documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff505edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre de clusters : {len(cluster_documents)}\")\n",
    "print(f\"Taille moyenne des documents : {cluster_documents['document'].str.len().mean():.0f} caract√®res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a876c47",
   "metadata": {},
   "source": [
    "### Construction de la matrice TF‚ÄëIDF\n",
    "\n",
    "On garde uniquement les mots utiles :\n",
    "- `min_df=2` : le mot doit appara√Ætre dans au moins 2 clusters\n",
    "- `max_df=0.8` : on supprime les mots trop communs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(cluster_documents['document'])\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print(f\"Matrice TF‚ÄëIDF : {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70092df3",
   "metadata": {},
   "source": [
    "### Extraction des top mots par cluster\n",
    "\n",
    "On cr√©e une fonction r√©utilisable pour extraire les mots les plus repr√©sentatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_terms_for_cluster(cluster_idx, top_n=3):\n",
    "    scores = tfidf_matrix[cluster_idx].toarray().flatten()\n",
    "    top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "    return feature_names[top_indices]\n",
    "\n",
    "# Associer les top mots √† chaque cluster\n",
    "tfidf_labels = {}\n",
    "\n",
    "for i in range(len(cluster_documents)):\n",
    "    cluster_id = cluster_documents.iloc[i]['cluster']\n",
    "    top_terms = get_top_terms_for_cluster(i, top_n=4)\n",
    "    tfidf_labels[cluster_id] = \" / \".join(top_terms)\n",
    "\n",
    "# Ajouter au DataFrame\n",
    "df['cluster_tfidf'] = df['cluster_hdbscan'].map(tfidf_labels).fillna(\"noise\")\n",
    "\n",
    "print(\"‚úì Mots-cl√©s TF‚ÄëIDF ajout√©s au dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c23da7a",
   "metadata": {},
   "source": [
    "### Visualisation des clusters (r√©sum√© final)\n",
    "\n",
    "On visualise les clusters sur une carte avec leurs labels TF‚ÄëIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700065b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# √âchantillonner pour la performance (comme avant)\n",
    "sample_size = 30000\n",
    "sample = df.sample(n=min(sample_size, len(df)), random_state=0)\n",
    "\n",
    "m = folium.Map(\n",
    "    location=[df[\"lat\"].median(), df[\"long\"].median()],\n",
    "    zoom_start=12,\n",
    "    tiles=\"CartoDB positron\"\n",
    ")\n",
    "\n",
    "palette = [\n",
    "    \"red\", \"blue\", \"green\", \"purple\", \"orange\",\n",
    "    \"darkred\", \"lightred\", \"beige\", \"darkblue\",\n",
    "    \"darkgreen\", \"cadetblue\", \"darkpurple\",\n",
    "    \"pink\", \"lightblue\", \"lightgreen\",\n",
    "    \"gray\", \"black\", \"lightgray\"\n",
    "]\n",
    "\n",
    "for _, r in sample.iterrows():\n",
    "    cluster = r[\"cluster_hdbscan\"]\n",
    "    if cluster == -1:\n",
    "        continue\n",
    "    \n",
    "    color = palette[cluster % len(palette)]\n",
    "    label = r[\"cluster_tfidf\"]\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[r[\"lat\"], r[\"long\"]],\n",
    "        radius=2,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.6,\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>TF‚ÄëIDF:</b> {label}<br/>\n",
    "               <b>Cluster:</b> {cluster}<br/>\n",
    "               <a href=\"{r.get('url', '#')}\" target=\"_blank\">Open Flickr</a>\"\"\",\n",
    "            max_width=250\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "title_html = '''\n",
    "<div style=\"position: fixed; top: 10px; left: 50px; width: 400px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "     <b>M√©thode 2 : TF‚ÄëIDF (top mots par cluster)</b>\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "m.save(\"output/text_mining_tfidf.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fe447",
   "metadata": {},
   "source": [
    "### Analyse des r√©sultats\n",
    "\n",
    "**Points forts** :\n",
    "- Les mots sont souvent plus sp√©cifiques que la m√©thode 1.\n",
    "- Moins d√©pendant des stopwords manuels.\n",
    "\n",
    "**Limites** :\n",
    "- La qualit√© d√©pend de la pr√©paration (cleaning + stopwords).\n",
    "\n",
    "TF‚ÄëIDF donne une base solide pour nommer les clusters, mais il faut toujours ajuster avec une inspection manuelle pour ajuster les stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8d6a5",
   "metadata": {},
   "source": [
    "## 4. M√©thode 3 : R√®gles d'Association"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566eb4ed",
   "metadata": {},
   "source": [
    "### Principe\n",
    "\n",
    "Les **r√®gles d'association** d√©couvrent des **combinaisons de mots** qui apparaissent fr√©quemment ensemble dans les clusters.\n",
    "\n",
    "**Diff√©rence avec les m√©thodes pr√©c√©dentes :**\n",
    "- M√©thode 1 : un seul mot dominant ‚Üí ex: \"basilique\"\n",
    "- M√©thode 2 (TF-IDF) : liste de mots distinctifs ‚Üí ex: \"basilique, fourviere, colline\"\n",
    "- **M√©thode 3** : associations entre mots ‚Üí ex: **\"basilique + fourviere\"** (vont souvent ensemble)\n",
    "\n",
    "On peut ainsi capturer les noms compos√©s et expressions (\"place bellecour\", \"parc tete d'or\") plut√¥t que des mots isol√©s.\n",
    "\n",
    "On commence avec l'algorithme Apriori, qui g√©n√®re des r√®gles du type `{mot1, mot2} ‚Üí {mot3}` avec des scores de confiance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14f79a",
   "metadata": {},
   "source": [
    "### Pr√©paration des transactions\n",
    "\n",
    "On transforme chaque cluster en une \"transaction\" (liste de mots uniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les transactions : 1 transaction = tous les mots d'un cluster\n",
    "transactions = []\n",
    "\n",
    "for cluster_id in sorted(df['cluster_hdbscan'].unique()):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    \n",
    "    # R√©cup√©rer tous les mots du cluster\n",
    "    cluster_data = df[df['cluster_hdbscan'] == cluster_id]\n",
    "    all_words = []\n",
    "    \n",
    "    for words_list in cluster_data['cleaned_text'].dropna():\n",
    "        all_words.extend(words_list)\n",
    "    \n",
    "    # Garder les mots uniques\n",
    "    unique_words = list(set(all_words))\n",
    "    transactions.append(unique_words)\n",
    "\n",
    "print(f\"‚úì {len(transactions)} transactions cr√©√©es\")\n",
    "print(f\"  Exemple (cluster 0) : {transactions[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187ce2c",
   "metadata": {},
   "source": [
    "L'algorithme Apriori n√©cessite un format binaire : chaque ligne = 1 transaction, chaque colonne = 1 mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(f\"Matrice encod√©e : {df_encoded.shape}\")\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1b342",
   "metadata": {},
   "source": [
    "### Apriori avec plusieurs param√®tres\n",
    "\n",
    "Le **support** minimal d√©finit la fr√©quence minimale d'apparition d'une combinaison de mots.\n",
    "\n",
    "On teste plusieurs valeurs pour trouver le bon √©quilibre :\n",
    "- Trop bas ‚Üí trop de r√®gles (bruit)\n",
    "- Trop haut ‚Üí pas assez de r√®gles (perte d'info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ae075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import numpy as np\n",
    "\n",
    "support_values = [0.125, 0.15, 0.25, 0.5, 0.75]\n",
    "results = []\n",
    "\n",
    "for min_sup in support_values:\n",
    "    print(f\"D√©but pour min_support = {min_sup}\")\n",
    "    \n",
    "    frequent_itemsets = apriori(\n",
    "        df_encoded,\n",
    "        min_support=min_sup,\n",
    "        use_colnames=True,\n",
    "        max_len=3\n",
    "    )\n",
    "    \n",
    "    # Ajouter length\n",
    "    if len(frequent_itemsets) > 0:\n",
    "        frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(len)\n",
    "        max_len = frequent_itemsets['length'].max()\n",
    "    else:\n",
    "        max_len = 0\n",
    "    \n",
    "    # G√©n√©rer des r√®gles si possible\n",
    "    if len(frequent_itemsets) > 0:\n",
    "        rules = association_rules(\n",
    "            frequent_itemsets,\n",
    "            metric=\"confidence\",\n",
    "            min_threshold=0.6,\n",
    "            num_itemsets=len(transactions)\n",
    "        )\n",
    "        mean_lift = rules['lift'].mean() if len(rules) > 0 else 0\n",
    "        mean_conf = rules['confidence'].mean() if len(rules) > 0 else 0\n",
    "    else:\n",
    "        rules = pd.DataFrame()\n",
    "        mean_lift = 0\n",
    "        mean_conf = 0\n",
    "    \n",
    "    # Couverture : % de clusters ayant au moins un itemset de taille >=2\n",
    "    if len(frequent_itemsets) > 0:\n",
    "        itemsets_2plus = frequent_itemsets[frequent_itemsets['length'] >= 2]['itemsets']\n",
    "        covered = 0\n",
    "        for cluster_words in transactions:\n",
    "            if any(itemset.issubset(set(cluster_words)) for itemset in itemsets_2plus):\n",
    "                covered += 1\n",
    "        coverage = covered / len(transactions)\n",
    "    else:\n",
    "        coverage = 0\n",
    "    \n",
    "    results.append({\n",
    "        'min_support': min_sup,\n",
    "        'n_itemsets': len(frequent_itemsets),\n",
    "        'max_length': max_len,\n",
    "        'mean_lift': mean_lift,\n",
    "        'mean_confidence': mean_conf,\n",
    "        'coverage': coverage\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚Üí itemsets: {len(frequent_itemsets)} | rules: {len(rules)} | coverage: {coverage:.2f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedeef50",
   "metadata": {},
   "source": [
    "On visualise le r√©sultat de l'exploration avec plusieurs valeurs de param√®tres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df775d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "# 1) Quantit√© d‚Äôitemsets\n",
    "ax[0][0].plot(results_df['min_support'], results_df['n_itemsets'], marker='o', linewidth=2)\n",
    "ax[0][0].set_title(\"Quantit√© d'itemsets\")\n",
    "ax[0][0].set_xlabel(\"min_support\")\n",
    "ax[0][0].set_ylabel(\"Nombre d'itemsets\")\n",
    "ax[0][0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2) Lift\n",
    "ax[0][1].plot(results_df['min_support'], results_df['mean_lift'], marker='s', label='Lift moyen')\n",
    "ax[0][1].set_title(\"Qualit√© des r√®gles : Lift\")\n",
    "ax[0][1].set_xlabel(\"min_support\")\n",
    "ax[0][1].set_ylabel(\"Valeur moyenne\")\n",
    "ax[0][1].legend()\n",
    "ax[0][1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3) Confidence\n",
    "ax[1][0].plot(results_df['min_support'], results_df['mean_confidence'], marker='^', label='Confidence moyenne')\n",
    "ax[1][0].set_title(\"Qualit√© des r√®gles : Confidence\")\n",
    "ax[1][0].set_xlabel(\"min_support\")\n",
    "ax[1][0].set_ylabel(\"Valeur moyenne\")\n",
    "ax[1][0].legend()\n",
    "ax[1][0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4) Couverture\n",
    "ax[1][1].plot(results_df['min_support'], results_df['coverage'], marker='o', color='green')\n",
    "ax[1][1].set_title(\"Couverture des clusters\")\n",
    "ax[1][1].set_xlabel(\"min_support\")\n",
    "ax[1][1].set_ylabel(\"Proportion couverte\")\n",
    "ax[1][1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84b647",
   "metadata": {},
   "source": [
    "### Analyse des graphiques et choix des param√®tres\n",
    "\n",
    "L'analyse des courbes ci-dessus met en √©vidence trois compromis majeurs pour le choix du `min_support` :\n",
    "\n",
    "1.  **Explosion combinatoire (Quantit√©)** : Le nombre d'itemsets diminue de mani√®re exponentielle. Avec un support de **0.10**, on obtient pr√®s de 300 000 itemsets, ce qui est tr√®s co√ªteux en calcul et contient probablement beaucoup de \"bruit\". √Ä **0.20**, ce nombre chute √† environ 20 000, rendant l'analyse beaucoup plus rapide.\n",
    "\n",
    "2.  **Qualit√© des r√®gles (Lift & Confidence)** : On observe que la qualit√© moyenne des r√®gles diminue lorsque le support augmente. \n",
    "    *   Les associations tr√®s fr√©quentes (support √©lev√©) sont souvent g√©n√©riques, d'o√π un *Lift* plus faible (~1.50).\n",
    "    *   Les associations plus rares (support faible) sont souvent plus fortes et sp√©cifiques (ex: \"basilique\" + \"fourviere\"), avec un *Lift* plus √©lev√© (~1.90).\n",
    "\n",
    "3.  **Couverture des clusters** : La couverture reste excellente (> 96%) jusqu'√† un support de **0.20**. Elle ne commence √† chuter significativement qu'apr√®s 0.20.\n",
    "\n",
    "**Conclusion** : \n",
    "Le point d'√©quilibre (\"le coude\") semble se situer autour de **0.20**.\n",
    "*   On √©limine l'explosion combinatoire (division par 6 du nombre d'itemsets).\n",
    "*   On maintient une excellente couverture (~96.5% des clusters sont labellis√©s).\n",
    "*   On conserve une qualit√© de r√®gles acceptable.\n",
    "\n",
    "Nous allons donc retenir **`min_support = 0.20`** pour la g√©n√©ration finale des r√®gles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4f6aa",
   "metadata": {},
   "source": [
    "### G√©n√©ration des r√®gles d'association\n",
    "\n",
    "On g√©n√®re les r√®gles avec une confiance minimale de 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# G√©n√©rer les itemsets fr√©quents avec min_support = 0.5\n",
    "frequent_itemsets = apriori(\n",
    "    df_encoded,\n",
    "    min_support=0.5,\n",
    "    use_colnames=True,\n",
    "    max_len=10\n",
    ")\n",
    "\n",
    "# Puis g√©n√©rer les r√®gles\n",
    "rules = association_rules(\n",
    "    frequent_itemsets, \n",
    "    metric=\"confidence\", \n",
    "    min_threshold=0.5, \n",
    "    num_itemsets=len(transactions)\n",
    ")\n",
    "rules = rules.sort_values('lift', ascending=False)\n",
    "\n",
    "print(f\"‚úì {len(rules)} r√®gles g√©n√©r√©es\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(100).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343045b6",
   "metadata": {},
   "source": [
    "### Top r√®gles par lift\n",
    "\n",
    "Le **lift** mesure la force de l'association (lift > 1 = association positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 r√®gles\n",
    "top_rules = rules.head(100).copy()\n",
    "\n",
    "# Formater pour l'affichage\n",
    "top_rules['rule'] = top_rules.apply(\n",
    "    lambda row: f\"{set(row['antecedents'])} ‚Üí {set(row['consequents'])}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(top_rules[['rule', 'support', 'confidence', 'lift']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b7566",
   "metadata": {},
   "source": [
    "Au vu de la faible qualit√© des r√®gles d'association trouv√©es, nous n'utiliserons pas cette m√©thode pour labelliser les clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806683a",
   "metadata": {},
   "source": [
    "# 5. LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc64d8",
   "metadata": {},
   "source": [
    "Au vu des mauvais r√©sultats donn√©s par les r√®gles d'association, on tente l'utilisation d'un LLM pour labelliser les clusters, √† partir des 5 mots les plus significatifs extraits par TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_title_with_ollama(keywords_list, model=\"mistral\"):\n",
    "    keywords_str = \", \".join(keywords_list)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Tu es un assistant utile pour nommer des clusters touristiques.\n",
    "    √Ä partir de ces mots-cl√©s d√©crivant un lieu √† Lyon (France) : \"{keywords_str}\".\n",
    "    G√©n√®re un titre tr√®s court et naturel (max 5 mots) en fran√ßais.\n",
    "    Le titre doit refl√©ter le nom du lieu, sans s'attacher aux mots-cl√©s g√©n√©riques\n",
    "    qui ne sont pas sp√©cifiques √† ce lieu.\n",
    "    **Retourne UNIQUEMENT le titre, sans guillemets ni explications.**\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['response'].strip()\n",
    "        else:\n",
    "            return \"Error LLM\"\n",
    "    except Exception as e:\n",
    "        return f\"Connection Failed: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc5ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Dictionary to store LLM generated titles\n",
    "llm_titles = {}\n",
    "\n",
    "# Get unique cluster IDs (excluding noise -1)\n",
    "unique_clusters = sorted(df['cluster_hdbscan'].unique())\n",
    "if -1 in unique_clusters: unique_clusters.remove(-1)\n",
    "\n",
    "print(f\"Generating titles for {len(unique_clusters)} clusters using Ollama...\")\n",
    "\n",
    "for cluster_id in tqdm(unique_clusters):\n",
    "    # Retrieve the top terms using your existing function index logic\n",
    "    # Note: Ensure the index matches logical position in cluster_documents\n",
    "    try:\n",
    "        # Find the index in cluster_documents corresponding to this cluster_id\n",
    "        doc_idx = cluster_documents.index[cluster_documents['cluster'] == cluster_id][0]\n",
    "        keywords = get_top_terms_for_cluster(doc_idx, top_n=4)\n",
    "        \n",
    "        # Generate title\n",
    "        human_title = generate_title_with_ollama(keywords)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(f\"Cluster {cluster_id}: Keywords: {keywords} -> Title: {human_title}\")\n",
    "        \n",
    "        llm_titles[cluster_id] = human_title\n",
    "    except IndexError:\n",
    "        llm_titles[cluster_id] = \"unknown\"\n",
    "\n",
    "# Add to DataFrame\n",
    "df['cluster_llm_title'] = df['cluster_hdbscan'].map(llm_titles).fillna(\"noise\")\n",
    "\n",
    "print(\"‚úì LLM Titles generated!\")\n",
    "print(df[['cluster_hdbscan', 'cluster_tfidf', 'cluster_llm_title']].drop_duplicates().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e767f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# √âchantillonner pour la performance\n",
    "sample_size = 30000\n",
    "sample = df.sample(n=min(sample_size, len(df)), random_state=0)\n",
    "\n",
    "m_llm = folium.Map(\n",
    "    location=[df[\"lat\"].median(), df[\"long\"].median()],\n",
    "    zoom_start=12,\n",
    "    tiles=\"CartoDB positron\"\n",
    ")\n",
    "\n",
    "# Palette de couleurs\n",
    "palette = [\n",
    "    \"red\", \"blue\", \"green\", \"purple\", \"orange\",\n",
    "    \"darkred\", \"lightred\", \"beige\", \"darkblue\",\n",
    "    \"darkgreen\", \"cadetblue\", \"darkpurple\",\n",
    "    \"pink\", \"lightblue\", \"lightgreen\",\n",
    "    \"gray\", \"black\", \"lightgray\"\n",
    "]\n",
    "\n",
    "for _, r in sample.iterrows():\n",
    "    cluster = r[\"cluster_hdbscan\"]\n",
    "    if cluster == -1:\n",
    "        continue\n",
    "    \n",
    "    color = palette[cluster % len(palette)]\n",
    "    \n",
    "    # On r√©cup√®re le titre LLM et les mots-cl√©s TF-IDF\n",
    "    llm_title = r.get(\"cluster_llm_title\", \"Unknown\")\n",
    "    tfidf_kw = r.get(\"cluster_tfidf\", \"N/A\")\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[r[\"lat\"], r[\"long\"]],\n",
    "        radius=2,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.6,\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>Title:</b> {llm_title}<br/>\n",
    "               <span style='font-size:0.8em; color:gray'>Keywords: {tfidf_kw}</span><br/>\n",
    "               <b>Cluster:</b> {cluster}<br/>\n",
    "               <a href=\"{r.get('url', '#')}\" target=\"_blank\">Open Flickr</a>\"\"\",\n",
    "            max_width=300\n",
    "        )\n",
    "    ).add_to(m_llm)\n",
    "\n",
    "title_html = '''\n",
    "<div style=\"position: fixed; top: 10px; left: 50px; width: 450px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "     <b>M√©thode 3 : Titres g√©n√©r√©s par IA (Ollama)</b><br>\n",
    "     <i style=\"font-size:12px\">Bas√© sur les mots-cl√©s TF-IDF</i>\n",
    "</div>\n",
    "'''\n",
    "m_llm.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "m_llm.save(\"output/text_mining_llm.html\")\n",
    "print(\"Carte sauvegard√©e dans output/text_mining_llm.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
