{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9932d9",
   "metadata": {},
   "source": [
    "# 1 - Exploration et nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fab7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb879f3d",
   "metadata": {},
   "source": [
    "On commence par créer un dataframe à partir du fichier CSV fourni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('flickr_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abcd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777b7e8",
   "metadata": {},
   "source": [
    "On enlève les espaces en début de nom de colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55865485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97942f41",
   "metadata": {},
   "source": [
    "## Colonnes de dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f6a6d",
   "metadata": {},
   "source": [
    "Les colonnes de dates sont converties en numérique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2660092",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\n",
    "    \"date_taken_minute\", \"date_taken_hour\", \"date_taken_day\",\n",
    "    \"date_taken_month\", \"date_taken_year\",\n",
    "    \"date_upload_minute\", \"date_upload_hour\", \"date_upload_day\",\n",
    "    \"date_upload_month\", \"date_upload_year\",\n",
    "]\n",
    "\n",
    "for c in date_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123151d",
   "metadata": {},
   "source": [
    "On s'intéresse à la cohérence de ces colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_rules = {\n",
    "    \"date_taken_minute\": (0, 59),\n",
    "    \"date_taken_hour\": (0, 23),\n",
    "    \"date_taken_day\": (1, 31),\n",
    "    \"date_taken_month\": (1, 12),\n",
    "    \"date_taken_year\": (1900, 2026),\n",
    "\n",
    "    \"date_upload_minute\": (0, 59),\n",
    "    \"date_upload_hour\": (0, 23),\n",
    "    \"date_upload_day\": (1, 31),\n",
    "    \"date_upload_month\": (1, 12),\n",
    "    \"date_upload_year\": (1900, 2026),\n",
    "}\n",
    "\n",
    "outlier_mask = pd.Series(False, index=df.index)\n",
    "\n",
    "for col, (lo, hi) in date_rules.items():\n",
    "    outlier_mask |= ~df[col].between(lo, hi)\n",
    "\n",
    "df_outliers = df[outlier_mask]\n",
    "df_outliers.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0136ce",
   "metadata": {},
   "source": [
    "Plusieurs colonnes contiennent des valeurs aberrantes (trop petites ou trop grandes) qui ne correspondent pas à des dates valides.\n",
    "On décide de créer de nouvelles colonnes de type datetime : les lignes avec des valeurs aberrantes seront converties en NaT (Not a Time).\n",
    "\n",
    "Si on a besoin de faire des analyses sur la date, on les fera en utilisant ces nouvelles colonnes nettoyées.\n",
    "On ne supprime pas les lignes avec des valeurs aberrantes, car elles peuvent contenir des informations utiles dans d'autres colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"taken_dt\"] = pd.to_datetime(\n",
    "    dict(\n",
    "        year=df[\"date_taken_year\"],\n",
    "        month=df[\"date_taken_month\"],\n",
    "        day=df[\"date_taken_day\"],\n",
    "        hour=df[\"date_taken_hour\"],\n",
    "        minute=df[\"date_taken_minute\"],\n",
    "    ),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "df[\"upload_dt\"] = pd.to_datetime(\n",
    "    dict(\n",
    "        year=df[\"date_upload_year\"],\n",
    "        month=df[\"date_upload_month\"],\n",
    "        day=df[\"date_upload_day\"],\n",
    "        hour=df[\"date_upload_hour\"],\n",
    "        minute=df[\"date_upload_minute\"],\n",
    "    ),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "df[[\"taken_dt\", \"upload_dt\"]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053bfd9",
   "metadata": {},
   "source": [
    "## Colonnes Unnamed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755f87f",
   "metadata": {},
   "source": [
    "On décide de supprimer les colonnes inutiles \"Unnamed: 16\", \"Unnamed: 17\" et \"Unnamed: 18\" car elles ne contiennent aucune information pertinente pour notre analyse, et sont globalement vides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8749b3a",
   "metadata": {},
   "source": [
    "## Lignes dupliquées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911cd8a3",
   "metadata": {},
   "source": [
    "Maintenant que l'on a supprimé les colonnes inutiles, on peut vérifier la présence de lignes dupliquées dans le DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22044267",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row_dupes = df.duplicated().sum()\n",
    "n_row_dupes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd75c39f",
   "metadata": {},
   "source": [
    "Il y a 252 139 lignes dupliquées dans le DataFrame. On les supprime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd200eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates() # Garde la première occurence\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7838d",
   "metadata": {},
   "source": [
    "On tombe à 168 101 lignes uniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d09ca",
   "metadata": {},
   "source": [
    "## Colonnes latitude, longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b63b2f",
   "metadata": {},
   "source": [
    "On vérifie la cohérence des colonnes \"latitude\" et \"longitude\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0eaef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"lat\", \"long\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d871846",
   "metadata": {},
   "source": [
    "On veut visualiser le carré encadrant toutes les coordonnées GPS présentes dans le dataset, pour voir si toutes les données sont bien à Lyon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On veut visualiser le carré encadrant toutes les coordonnées GPS présentes dans le dataset sur une carte\n",
    "min_lat, max_lat = df[\"lat\"].min(), df[\"lat\"].max()\n",
    "min_long, max_long = df[\"long\"].min(), df[\"long\"].max()\n",
    "\n",
    "import folium\n",
    "m = folium.Map(location=[(min_lat + max_lat) / 2, (min_long + max_long) / 2], zoom_start=2)\n",
    "m.fit_bounds([[min_lat, min_long], [max_lat, max_long]])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb31417",
   "metadata": {},
   "source": [
    "Toutes les données sont à Lyon et ses environs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a995973",
   "metadata": {},
   "source": [
    "## Colonnes title, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718eb92",
   "metadata": {},
   "source": [
    "On s'intéresse maintenant aux colonnes title et tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"title\", \"tags\"]].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b0a5e",
   "metadata": {},
   "source": [
    "On commence par un nettoyage basique de ces colonnes :\n",
    "- On les convertit en type string\n",
    "- On enlève les espaces superflus au début et fin\n",
    "- On remplace les espaces multiples par un seul dans la colonne \"title\" et \"tags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"title\", \"tags\", \"user\"]:\n",
    "    df[c] = df[c].astype(\"string\")\n",
    "    df[c] = df[c].fillna(\"\").str.strip()\n",
    "df[\"title\"] = df[\"title\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "df[\"tags\"] = df[\"tags\"].str.replace(r\"\\s+\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ca292",
   "metadata": {},
   "source": [
    "On crée une nouvelle colonne \"title_tags\" qui concatène les colonnes \"title\" et \"tags\", séparées par des virgules.\n",
    "\n",
    "Les tags qui contiennent des espaces sont séparés en plusieurs tags.\n",
    "\n",
    "Par exemple :\n",
    "\"titre,tag1,tag2,tag3,composé\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0870681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_tags\"] = (\n",
    "    df[\"title\"].str.replace(\" \", \",\") + \",\" + \n",
    "    df[\"tags\"].str.replace(\" \", \",\")\n",
    ")\n",
    "df[\"title_tags\"].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a7bb2",
   "metadata": {},
   "source": [
    "Le text mining sera basé sur cette colonne. Un nettoyage plus avancé sera fait plus tard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f68f9",
   "metadata": {},
   "source": [
    "## Colonne URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6230a491",
   "metadata": {},
   "source": [
    "On ajoute la colonne \"url\" qui sera utile pour accéder aux images et examiner le contenu du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"url\"] = (\n",
    "    \"https://www.flickr.com/photos/\"\n",
    "    + df[\"user\"].astype(str)\n",
    "    + \"/\"\n",
    "    + df[\"id\"].astype(str)\n",
    ")\n",
    "df[\"url\"].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43f4d5",
   "metadata": {},
   "source": [
    "# Export du DataFrame nettoyé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34392710",
   "metadata": {},
   "source": [
    "On exporte le dataframe nettoyé dans un fichier .parquet pour une utilisation dans les étapes suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c985954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export en parquet dans /output\n",
    "df.to_parquet(\"output/data_cleaned.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
