{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20211e93",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb7b19",
   "metadata": {},
   "source": [
    "## 1. K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688acc86",
   "metadata": {},
   "source": [
    "### Préparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbcc16",
   "metadata": {},
   "source": [
    "On suit la même démarche que celle utilisée en TP pour tenter un premier clustering avec k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145368ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"data_cleaned.parquet\")\n",
    "df = df.dropna(subset=[\"lat\", \"long\"]).copy()\n",
    "\n",
    "df_kmeans = df[[\"lat\", \"long\"]].copy()\n",
    "\n",
    "df_kmeans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba352cd",
   "metadata": {},
   "source": [
    "On standardise les données géographiques (lat, long) afin que le clustering ne soit pas biaisé par l'échelle des coordonnées. Ainsi, la latitude et la longitude auront la même importance dans le calcul des distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_kmeans)\n",
    "\n",
    "scaled_data_df = pd.DataFrame(scaled_data, columns=df_kmeans.columns, index=df_kmeans.index)\n",
    "scaled_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542e73dc",
   "metadata": {},
   "source": [
    "### Fonctions de base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495a465",
   "metadata": {},
   "source": [
    "On définit nos deux fonctions permettant d'exécuter le clustering et de le visualiser.\n",
    "\n",
    "La fonction run_kmeans_clustering exécute le clustering k-means pour une valeur donnée de k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebac07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def run_kmeans_clustering(df, scaled_data_df, scaler, k):\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=0)\n",
    "    kmeans.fit(scaled_data_df)\n",
    "    \n",
    "    df[\"cluster_kmeans\"] = kmeans.labels_\n",
    "    \n",
    "    # Compute centers in original space\n",
    "    centers_scaled = kmeans.cluster_centers_\n",
    "    centers = scaler.inverse_transform(centers_scaled)\n",
    "    centers_df = pd.DataFrame(centers, columns=[\"lat\", \"long\"])\n",
    "    \n",
    "    return centers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f2884",
   "metadata": {},
   "source": [
    "On définit la fonction create_clustering_map qui crée une carte permettant de visualiser les clusters obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from scipy.spatial import ConvexHull\n",
    "import numpy as np\n",
    "\n",
    "def create_clustering_map(df, cluster_col, centers_df=None, sample_size=5000, zoom_start=12):    \n",
    "    sample = df.sample(n=min(sample_size, len(df)), random_state=0)\n",
    "    \n",
    "    palette = [\n",
    "        \"red\", \"blue\", \"green\", \"purple\", \"orange\",\n",
    "        \"darkred\", \"lightred\", \"beige\", \"darkblue\",\n",
    "        \"darkgreen\", \"cadetblue\", \"darkpurple\",\n",
    "        \"pink\", \"lightblue\", \"lightgreen\",\n",
    "        \"gray\", \"black\", \"lightgray\"\n",
    "    ]\n",
    "    \n",
    "    m = folium.Map(\n",
    "        location=[df[\"lat\"].median(), df[\"long\"].median()],\n",
    "        zoom_start=zoom_start,\n",
    "        tiles=\"CartoDB positron\",\n",
    "    )\n",
    "    \n",
    "    # Dessiner les contours des clusters\n",
    "    for cluster_id in df[cluster_col].unique():\n",
    "        if cluster_id == -1:  # Ignorer le bruit\n",
    "            continue\n",
    "        cluster_points = df[df[cluster_col] == cluster_id][[\"lat\", \"long\"]].values\n",
    "        if len(cluster_points) > 2:\n",
    "            try:\n",
    "                hull = ConvexHull(cluster_points)\n",
    "                hull_coords = cluster_points[hull.vertices]\n",
    "                color = palette[cluster_id % len(palette)]\n",
    "                folium.PolyLine(\n",
    "                    locations=[[lat, lon] for lat, lon in hull_coords],\n",
    "                    color=color,\n",
    "                    weight=2,\n",
    "                    opacity=0.8\n",
    "                ).add_to(m)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Points d'échantillon (moins nombreux)\n",
    "    for _, r in sample.iterrows():\n",
    "        c = int(r[cluster_col])\n",
    "        color = palette[c % len(palette)]\n",
    "        folium.CircleMarker(\n",
    "            location=[r[\"lat\"], r[\"long\"]],\n",
    "            radius=2,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_opacity=0.5,\n",
    "            popup=folium.Popup(f\"\"\"<a href=\"{r.get('url','')}\" target=\"_blank\">Open Flickr</a>\"\"\", max_width=250),\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Centres des clusters\n",
    "    if centers_df is not None:\n",
    "        for i, row in centers_df.iterrows():\n",
    "            folium.Marker(\n",
    "                location=[row[\"lat\"], row[\"long\"]],\n",
    "                icon=folium.Icon(color=\"darkblue\", icon=\"star\"),\n",
    "                popup=f\"Centre du cluster {i}\",\n",
    "            ).add_to(m)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80800b",
   "metadata": {},
   "source": [
    "### K déterminé par la méthode du coude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0884e2e",
   "metadata": {},
   "source": [
    "On commence par déterminer le nombre optimal de clusters k en utilisant la méthode du coude. On calcule l'inertie pour différentes valeurs de k et on trace la courbe correspondante.\n",
    "\n",
    "L'inertie mesure la dispersion des points au sein des clusters : plus les points sont proches de leur centre de cluster, plus l'inertie est faible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89922fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inertia_values = []\n",
    "k_values = range(1, 15)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=0)\n",
    "    kmeans.fit(scaled_data_df)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(k_values), inertia_values, marker=\"o\")\n",
    "plt.title(\"Méthode du coude pour K-Means\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Inertie\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ee5e4",
   "metadata": {},
   "source": [
    "On trouve k = 4 avec la méthode du coude. On teste un clustering avec cette valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b955faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_df = run_kmeans_clustering(df, scaled_data_df, scaler, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = create_clustering_map(df, \"cluster_kmeans\", centers_df=centers_df)\n",
    "m.save(\"output/kmeans/kmeans4.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa80af",
   "metadata": {},
   "source": [
    "Le résultat obtenu avec k = 4 n'est pas satisfaisant. La méthode du coude nous donne une valeur de k qui ne produit de clustering pertinent.\n",
    "\n",
    "Dans Lyon, il n'y a pas que 4 zones d'intérêt. Il faudrait essayer avec un k plus grand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664ff17",
   "metadata": {},
   "source": [
    "### Autres valeurs de K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [10, 50, 150, 200]:\n",
    "    centers_df = run_kmeans_clustering(df, scaled_data_df, scaler, k)\n",
    "    m = create_clustering_map(df, \"cluster_kmeans\", centers_df=centers_df)\n",
    "    m.save(f\"output/kmeans/kmeans{k}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71234f46",
   "metadata": {},
   "source": [
    "Même avec d'autres valeurs de k, le clustering n'est pas vraiment pertinent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6989c23",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "\n",
    "Même avec d'autres valeurs de k, le clustering K-means n'est pas adapté à notre cas d'usage.\n",
    "\n",
    "**Limites observées** :\n",
    "- K-means **partitionne l'espace géographique** de manière uniforme plutôt que d'identifier les zones de forte densité\n",
    "- Les clusters sont **forcés d'être sphériques** (distances euclidiennes), inadapté aux hotspots touristiques de formes irrégulières\n",
    "- **Sensibilité au bruit** : les points isolés influencent les centres des clusters\n",
    "- **Nécessité de fixer k a priori** sans garantie sur la pertinence du nombre de zones d'intérêt\n",
    "\n",
    "K-means privilégie une **couverture territoriale homogène** plutôt que la détection de **hotspots denses**, ce qui ne répond pas à notre objectif d'optimisation des transports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846678d8",
   "metadata": {},
   "source": [
    "## 2.2 DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2dbee2",
   "metadata": {},
   "source": [
    "Suite à l'essai de k-means, on tente un clustering avec DBSCAN.\n",
    "\n",
    "Le principe de DBSCAN est de regrouper les points denses ensemble, et de considérer les points isolés comme du bruit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856c5f1",
   "metadata": {},
   "source": [
    "### Préparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eebfe0",
   "metadata": {},
   "source": [
    "On importe les données nettoyées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"output/data_cleaned.parquet\")\n",
    "df = df.dropna(subset=[\"lat\", \"long\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "coords = df[[\"lat\", \"long\"]].to_numpy()\n",
    "coords_rad = np.radians(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca8f66",
   "metadata": {},
   "source": [
    "### Fonctions de base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fc476",
   "metadata": {},
   "source": [
    "On crée une fonction DBSCAN pour pouvoir l'appliquer avec différents paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035f028",
   "metadata": {},
   "source": [
    "Afin de pouvoir utiliser un epsilon exprimé en mètres, on convertit les coordonnées géographiques (lat, long) en coordonnées cartésiennes, en utilisant haversine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def run_dbscan(coords_rad, eps_meters, min_samples):\n",
    "    # conversion mètres → radians\n",
    "    eps_rad = eps_meters / 6371000\n",
    "\n",
    "    db = DBSCAN(\n",
    "        eps=eps_rad,\n",
    "        min_samples=min_samples,\n",
    "        metric=\"haversine\",\n",
    "        algorithm=\"ball_tree\" # Plus rapide pour haversine\n",
    "    )\n",
    "    labels = db.fit_predict(coords_rad)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd347ea",
   "metadata": {},
   "source": [
    "On crée une fonction qui nous permettra de visualiser les résultats du clustering DBSCAN sur une carte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "def plot_dbscan_map(df, labels, title, sample_size=5000):\n",
    "    dff = df.copy()\n",
    "    dff[\"cluster\"] = labels\n",
    "\n",
    "    # échantillon\n",
    "    if len(dff) > sample_size:\n",
    "        dff = dff.sample(sample_size, random_state=0)\n",
    "\n",
    "    m = folium.Map(\n",
    "        location=[dff[\"lat\"].median(), dff[\"long\"].median()],\n",
    "        zoom_start=12,\n",
    "        tiles=\"CartoDB positron\"\n",
    "    )\n",
    "\n",
    "    palette = [\n",
    "        \"red\",\"blue\",\"green\",\"purple\",\"orange\",\"darkred\",\"cadetblue\",\n",
    "        \"darkgreen\",\"darkpurple\",\"pink\",\"gray\",\"black\"\n",
    "    ]\n",
    "\n",
    "    for _, r in dff.iterrows():\n",
    "        if r[\"cluster\"] == -1:\n",
    "            color = \"lightgray\"\n",
    "        else:\n",
    "            color = palette[r[\"cluster\"] % len(palette)]\n",
    "\n",
    "        folium.CircleMarker(\n",
    "            location=[r[\"lat\"], r[\"long\"]],\n",
    "            radius=2,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "        ).add_to(m)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f837296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def dbscan_stats(coords_rad, labels):\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    # Silhouette uniquement sur points non-bruit\n",
    "    mask = labels != -1\n",
    "    sil = None\n",
    "    if mask.sum() > 1 and n_clusters > 1:\n",
    "        sil = silhouette_score(coords_rad[mask], labels[mask], metric=\"euclidean\")\n",
    "\n",
    "    return {\n",
    "        \"clusters\": n_clusters,\n",
    "        \"noise_ratio\": n_noise / len(labels),\n",
    "        \"silhouette\": sil,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1fd5f1",
   "metadata": {},
   "source": [
    "### Détermination des paramètres epsilon et min_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a677c5",
   "metadata": {},
   "source": [
    "#### Min_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543f6eb",
   "metadata": {},
   "source": [
    "La littérature indique que min_samples peut être fixé à 4 pour des données en 2D. On commence donc notre analyse avec cette valeur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d38ac",
   "metadata": {},
   "source": [
    "#### Epsilon déterminé avec la méthode du coude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231559d3",
   "metadata": {},
   "source": [
    "On veut déterminer une bonne valeur pour epsilon en utilisant la méthode du coude.\n",
    "Pour cela, on calcule la distance au 4ème plus proche voisin pour chaque point, puis on trace ces distances triées.\n",
    "\n",
    "4 correspond au min_samples que l'on souhaite utiliser pour DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def coude_dbscan(min_samples, coords_rad):\n",
    "    k = min_samples  # min_samples value\n",
    "    neighbors = NearestNeighbors(n_neighbors=k)\n",
    "    neighbors_fit = neighbors.fit(coords_rad)\n",
    "    distances, indices = neighbors_fit.kneighbors(coords_rad)\n",
    "\n",
    "    distances = np.sort(distances[:, k-1], axis=0)\n",
    "\n",
    "    # Tracer la courbe\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(distances)\n",
    "    plt.ylabel(f'Distance au {k}-ième plus proche voisin (radians)')\n",
    "    plt.xlabel('Points (triés par distance)')\n",
    "    plt.title('Méthode du coude pour déterminer epsilon (DBSCAN)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    return plt\n",
    "\n",
    "coude_plot = coude_dbscan(min_samples=4, coords_rad=coords_rad)\n",
    "coude_plot.axhline(y=90/6371000, color='r', linestyle='--', label='90m')\n",
    "coude_plot.legend()\n",
    "coude_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c735a63",
   "metadata": {},
   "source": [
    "On choisit donc de commencer notre analyse avec epsilon = 90m et min_samples = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a5f2d4",
   "metadata": {},
   "source": [
    "### Avec epsilon = 90m et min_samples = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd136214",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "labels = run_dbscan(coords_rad, 90, 4)\n",
    "plot_dbscan_map(df, labels, title=\"DBSCAN eps=90m / min_samples=4\").save(\"output/dbscan/dbscan_eps90_ms4.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8283c02",
   "metadata": {},
   "source": [
    "On observe :\n",
    "- Beaucoup de clusters peu pertinents (trop peu de points pour être considéré comme intéressants).\n",
    "- Des clusters beaucoup trop gros.\n",
    "\n",
    "On va donc essayer DBSCAN avec :\n",
    "- Des valeurs d'epsilon plus petites.\n",
    "- Des valeurs de min_samples plus grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af44c57",
   "metadata": {},
   "source": [
    "### En variant epsilon et min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for eps in [20, 40, 60]:\n",
    "    for ms in [5, 50, 100]:\n",
    "        labels = run_dbscan(coords_rad, eps, ms)\n",
    "        map = plot_dbscan_map(df, labels, title=f\"DBSCAN eps={eps}m / min_samples={ms}\")\n",
    "        map.save(f\"output/dbscan/dbscan_eps{eps}_ms{ms}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea654f",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "\n",
    "Les tests avec différentes combinaisons de paramètres révèlent **une limitation fondamentale de DBSCAN** pour notre cas d'usage.\n",
    "\n",
    "**Observations :**\n",
    "\n",
    "1. **Impact de min_samples** : Augmenter `min_samples` améliore significativement la pertinence des clusters en filtrant les zones peu denses. Un seuil élevé (50-100) permet d'identifier uniquement les véritables hotspots touristiques.\n",
    "\n",
    "2. **Dilemme du paramètre epsilon** : Le choix d'epsilon pose un problème structurel pour Lyon :\n",
    "   - **Epsilon élevé (~100m)** : Fusionne les points d'intérêt du centre-ville (ex: fusion de la Presqu'île en un seul mega-cluster), perdant la granularité nécessaire pour distinguer Place Bellecour, Place des Terreaux, etc.\n",
    "   - **Epsilon faible (~30m)** : Capture bien les petits clusters denses du centre, mais échoue à détecter les zones d'intérêt plus dispersées (Parc de la Tête d'Or, quartier de la Part-Dieu).\n",
    "\n",
    "**Conclusion** : DBSCAN nécessite un epsilon **uniforme** alors que Lyon présente des zones d'intérêt de **densités variables**. Cette rigidité empêche de capturer simultanément les petits clusters concentrés du centre-ville et les grands espaces touristiques périphériques.\n",
    "\n",
    "→ **Solution** : On utilise donc HDBSCAN qui adapte automatiquement la distance selon la densité locale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074a1c2",
   "metadata": {},
   "source": [
    "## 2.3 HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c3c22",
   "metadata": {},
   "source": [
    "HDBSCAN adapte la densité locale, il n'est donc pas nécessaire de fixer un epsilon global. On espère ainsi mieux capturer les zones d'intérêt de densités variées à Lyon.\n",
    "\n",
    "**Paramètre clé** : `min_cluster_size` détermine la taille minimale d'un cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f351a9e",
   "metadata": {},
   "source": [
    "### Préparation\n",
    "Pour HDBSCAN, on travaille en coordonnées cartésiennes (projection Web Mercator EPSG:3857) plutôt qu'en lat/long, pour avoir des distances en mètres. GeoPandas nous permet de faire cette conversion facilement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68992fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# Charger les données nettoyées\n",
    "df = pd.read_parquet(\"output/data_cleaned.parquet\")\n",
    "df = df.dropna(subset=[\"lat\", \"long\"]).copy()\n",
    "\n",
    "# Convertir en GeoDataFrame avec projection Web Mercator\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, \n",
    "    geometry=gpd.points_from_xy(df[\"long\"], df[\"lat\"]), \n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "\n",
    "# Extraire les coordonnées cartésiennes en mètres\n",
    "X_m = np.column_stack([gdf.geometry.x.to_numpy(), gdf.geometry.y.to_numpy()])\n",
    "\n",
    "print(f\"Données préparées : {len(X_m)} points avec coordonnées cartésiennes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efdcfb",
   "metadata": {},
   "source": [
    "### Fonctions de base\n",
    "On crée trois fonctions réutilisables :\n",
    "1. **run_hdbscan** : Exécute le clustering HDBSCAN\n",
    "2. **create_hdbscan_map** : Visualise les résultats sur une carte\n",
    "3. **hdbscan_stats** : Calcule les statistiques des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "def run_hdbscan(X_m, min_cluster_size, min_samples=None):\n",
    "    \"\"\"\n",
    "    Exécute HDBSCAN sur les coordonnées cartésiennes.\n",
    "    \n",
    "    Args:\n",
    "        X_m : Coordonnées en mètres (n_samples, 2)\n",
    "        min_cluster_size : Taille minimale d'un cluster\n",
    "        min_samples : Nombre minimal de points pour un core point (None = min_cluster_size)\n",
    "    \n",
    "    Returns:\n",
    "        labels : Étiquettes de cluster (-1 pour bruit)\n",
    "    \"\"\"\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "    )\n",
    "    labels = clusterer.fit_predict(X_m)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3766ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "def create_hdbscan_map(df, labels, title=\"HDBSCAN Clustering\", sample_size=1000, zoom_start=12):\n",
    "    \"\"\"\n",
    "    Crée une carte Folium pour visualiser les clusters HDBSCAN.\n",
    "    \n",
    "    Args:\n",
    "        df : DataFrame avec colonnes 'lat' et 'long'\n",
    "        labels : Tableau d'étiquettes de cluster\n",
    "        title : Titre de la visualisation\n",
    "        sample_size : Nombre de points à afficher (pour performance)\n",
    "        zoom_start : Niveau de zoom initial\n",
    "    \n",
    "    Returns:\n",
    "        m : Objet folium.Map\n",
    "    \"\"\"\n",
    "    dff = df.copy()\n",
    "    dff[\"cluster\"] = labels\n",
    "    \n",
    "    # Échantillonner si trop de points\n",
    "    if len(dff) > sample_size:\n",
    "        dff = dff.sample(n=sample_size, random_state=0)\n",
    "    \n",
    "    m = folium.Map(\n",
    "        location=[dff[\"lat\"].median(), dff[\"long\"].median()],\n",
    "        zoom_start=zoom_start,\n",
    "        tiles=\"CartoDB positron\"\n",
    "    )\n",
    "    \n",
    "    palette = [\n",
    "        \"red\", \"blue\", \"green\", \"purple\", \"orange\",\n",
    "        \"darkred\", \"lightred\", \"beige\", \"darkblue\",\n",
    "        \"darkgreen\", \"cadetblue\", \"darkpurple\",\n",
    "        \"pink\", \"lightblue\", \"lightgreen\",\n",
    "        \"gray\", \"black\", \"lightgray\"\n",
    "    ]\n",
    "    \n",
    "    for _, r in dff.iterrows():\n",
    "        if r[\"cluster\"] == -1:\n",
    "            # Ignorer les points de bruit\n",
    "            continue\n",
    "        else:\n",
    "            color = palette[r[\"cluster\"] % len(palette)]\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[r[\"lat\"], r[\"long\"]],\n",
    "            radius=2,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "            popup=folium.Popup(\n",
    "                f\"\"\"<a href=\"{r.get('url', '')}\" target=\"_blank\">Open Flickr</a>\"\"\",\n",
    "                max_width=250\n",
    "            )\n",
    "        ).add_to(m)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdbscan_stats(labels):\n",
    "    \"\"\"\n",
    "    Calcule les statistiques du clustering HDBSCAN.\n",
    "    \n",
    "    Args:\n",
    "        labels : Tableau d'étiquettes de cluster\n",
    "    \n",
    "    Returns:\n",
    "        dict avec les statistiques\n",
    "    \"\"\"\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_ratio = n_noise / len(labels) * 100\n",
    "    \n",
    "    return {\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"n_noise\": n_noise,\n",
    "        \"noise_ratio\": noise_ratio,\n",
    "        \"total_points\": len(labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f9ff2",
   "metadata": {},
   "source": [
    "### Détermination des paramètres\n",
    "Le paramètre clé est `min_cluster_size`. Une valeur trop faible crée du bruit, une valeur trop élevée écrase les petits clusters pertinents.\n",
    "\n",
    "On teste plusieurs valeurs pour trouver l'équilibre optimal : 20, 50, 100, 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df71846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester différentes valeurs de min_cluster_size\n",
    "results_hdbscan = []\n",
    "\n",
    "for min_cluster_size in [20, 50, 100, 200]:\n",
    "    labels = run_hdbscan(X_m, min_cluster_size=min_cluster_size)\n",
    "    stats = hdbscan_stats(labels)\n",
    "    results_hdbscan.append(stats)\n",
    "    \n",
    "    print(f\"\\nmin_cluster_size = {min_cluster_size}\")\n",
    "    print(f\"  Clusters : {stats['n_clusters']}\")\n",
    "    print(f\"  Points bruit : {stats['n_noise']} ({stats['noise_ratio']:.1f}%)\")\n",
    "    \n",
    "    # Générer et sauvegarder la carte\n",
    "    m = create_hdbscan_map(df, labels, title=f\"HDBSCAN min_cluster_size={min_cluster_size}\")\n",
    "    m.save(f\"output/hdbscan/hdbscan_mc{min_cluster_size}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd3edb",
   "metadata": {},
   "source": [
    "### Résultats\n",
    "Basé sur les statistiques et les visualisations, on choisit `min_cluster_size=100`, qui semble être le paramètre avec lequel HDBSCAN produit les meilleurs clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700c529",
   "metadata": {},
   "source": [
    "### Analyse des résultats et comparaison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a632a1a",
   "metadata": {},
   "source": [
    "**Comparaison des trois méthodes** :\n",
    "\n",
    "| Critère | K-means | DBSCAN | HDBSCAN |\n",
    "|---------|---------|--------|---------|\n",
    "| Adaptation densité variable | ❌ Non | ⚠️ Partiel | ✅ Oui |\n",
    "| Formes clusters | Sphériques | Quelconques | Quelconques |\n",
    "| Paramétrage | k requis | epsilon+ms | min_cluster_size |\n",
    "| Sensibilité bruit | Haute | Moyenne | Basse |\n",
    "| **Pertinence Lyon** | Faible | Moyen | **Excellente** |\n",
    "\n",
    "**Conclusion** : HDBSCAN capture efficacement les hotspots de Lyon en s'adaptant à leurs densités variables, tout en gérant bien le bruit. C'est donc notre meilleure option pour l'optimisation des transports touristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e677f8d",
   "metadata": {},
   "source": [
    "### Export des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cea390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les labels finaux au DataFrame\n",
    "optimal_min_cluster_size = 100\n",
    "labels_final = run_hdbscan(X_m, min_cluster_size=optimal_min_cluster_size)\n",
    "df[\"cluster_hdbscan\"] = labels_final\n",
    "\n",
    "# Exporter en Parquet pour utilisation ultérieure\n",
    "df.to_parquet(\"output/flickr_data_clustered.parquet\")\n",
    "\n",
    "print(f\"✓ Données exportées : {len(df)} points avec cluster_hdbscan\")\n",
    "print(f\"✓ Clusters identifiés : {final_stats['n_clusters']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
